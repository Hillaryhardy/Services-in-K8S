* 
* ==> Audit <==
* |---------|--------------------------------|----------|---------|---------|---------------------|---------------------|
| Command |              Args              | Profile  |  User   | Version |     Start Time      |      End Time       |
|---------|--------------------------------|----------|---------|---------|---------------------|---------------------|
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 04:20 EAT |                     |
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 04:27 EAT |                     |
| start   | --vm-driver=hyperkit           | minikube | spartan | v1.31.2 | 02 Sep 23 04:28 EAT |                     |
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 04:29 EAT |                     |
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 04:30 EAT |                     |
| start   | --vm-driver virtualbox         | minikube | spartan | v1.31.2 | 02 Sep 23 04:32 EAT |                     |
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 04:33 EAT |                     |
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 04:34 EAT |                     |
| start   | --vm-driver virtualbox         | minikube | spartan | v1.31.2 | 02 Sep 23 04:34 EAT |                     |
| start   | ‚Äî-vm-driver=none               | minikube | spartan | v1.31.2 | 02 Sep 23 04:46 EAT |                     |
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 05:08 EAT |                     |
| start   | --driver=docker                | minikube | spartan | v1.31.2 | 02 Sep 23 05:08 EAT |                     |
| delete  |                                | minikube | spartan | v1.31.2 | 02 Sep 23 05:09 EAT | 02 Sep 23 05:09 EAT |
| start   | --driver=docker                | minikube | spartan | v1.31.2 | 02 Sep 23 05:09 EAT |                     |
| delete  |                                | minikube | spartan | v1.31.2 | 02 Sep 23 12:29 EAT | 02 Sep 23 12:29 EAT |
| delete  |                                | minikube | spartan | v1.31.2 | 02 Sep 23 12:41 EAT | 02 Sep 23 12:41 EAT |
| stop    |                                | minikube | spartan | v1.31.2 | 02 Sep 23 15:15 EAT |                     |
| delete  |                                | minikube | spartan | v1.31.2 | 02 Sep 23 15:15 EAT | 02 Sep 23 15:15 EAT |
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 15:33 EAT |                     |
| delete  |                                | minikube | spartan | v1.31.2 | 02 Sep 23 15:35 EAT | 02 Sep 23 15:35 EAT |
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 15:35 EAT |                     |
| start   | --driver=docker                | minikube | spartan | v1.31.2 | 02 Sep 23 15:52 EAT |                     |
| delete  |                                | minikube | spartan | v1.31.2 | 02 Sep 23 15:52 EAT | 02 Sep 23 15:52 EAT |
| start   | --driver=docker                | minikube | spartan | v1.31.2 | 02 Sep 23 15:52 EAT |                     |
| start   | --driver=docker                | minikube | spartan | v1.31.2 | 02 Sep 23 16:01 EAT |                     |
| start   | --driver=docker                | minikube | spartan | v1.31.2 | 02 Sep 23 16:06 EAT |                     |
| start   | --driver=docker                | minikube | spartan | v1.31.2 | 02 Sep 23 16:38 EAT |                     |
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 17:21 EAT | 02 Sep 23 18:09 EAT |
| start   |                                | minikube | spartan | v1.31.2 | 02 Sep 23 18:13 EAT | 02 Sep 23 18:14 EAT |
| stop    |                                | minikube | spartan | v1.31.2 | 03 Sep 23 07:08 EAT | 03 Sep 23 07:09 EAT |
| start   |                                | minikube | spartan | v1.31.2 | 03 Sep 23 07:11 EAT | 03 Sep 23 07:12 EAT |
| stop    |                                | minikube | spartan | v1.31.2 | 03 Sep 23 14:05 EAT | 03 Sep 23 14:05 EAT |
| start   |                                | minikube | spartan | v1.31.2 | 03 Sep 23 14:06 EAT | 03 Sep 23 14:07 EAT |
| start   |                                | minikube | spartan | v1.31.2 | 04 Sep 23 12:20 EAT | 04 Sep 23 12:23 EAT |
| service | mongo-express-servic           | minikube | spartan | v1.31.2 | 04 Sep 23 16:21 EAT |                     |
| service | mongo-express-service          | minikube | spartan | v1.31.2 | 04 Sep 23 16:22 EAT |                     |
| service | mongodb-express-service        | minikube | spartan | v1.31.2 | 04 Sep 23 21:51 EAT |                     |
| service | mongo-express-service          | minikube | spartan | v1.31.2 | 04 Sep 23 21:51 EAT | 04 Sep 23 21:51 EAT |
| service | mongo-express-service          | minikube | spartan | v1.31.2 | 04 Sep 23 21:55 EAT | 04 Sep 23 21:55 EAT |
| addons  | enable ingress                 | minikube | spartan | v1.31.2 | 04 Sep 23 22:29 EAT |                     |
| stop    |                                | minikube | spartan | v1.31.2 | 04 Sep 23 23:05 EAT | 04 Sep 23 23:05 EAT |
| start   |                                | minikube | spartan | v1.31.2 | 09 Oct 23 09:02 EAT | 09 Oct 23 09:06 EAT |
| service | task-app-task-1 -n=task-appns  | minikube | spartan | v1.31.2 | 09 Oct 23 09:33 EAT |                     |
|         | --url                          |          |         |         |                     |                     |
|---------|--------------------------------|----------|---------|---------|---------------------|---------------------|

* 
* ==> Last Start <==
* Log file created at: 2023/10/09 09:02:40
Running on machine: spartan-HP-ProBook-450-G1
Binary: Built with gc go1.20.7 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I1009 09:02:40.899923   14250 out.go:296] Setting OutFile to fd 1 ...
I1009 09:02:40.900460   14250 out.go:348] isatty.IsTerminal(1) = true
I1009 09:02:40.900469   14250 out.go:309] Setting ErrFile to fd 2...
I1009 09:02:40.900480   14250 out.go:348] isatty.IsTerminal(2) = true
I1009 09:02:40.901076   14250 root.go:338] Updating PATH: /home/spartan/.minikube/bin
I1009 09:02:40.945156   14250 out.go:303] Setting JSON to false
I1009 09:02:41.000073   14250 start.go:128] hostinfo: {"hostname":"spartan-HP-ProBook-450-G1","uptime":48492,"bootTime":1696782869,"procs":278,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"6.2.0-34-generic","kernelArch":"x86_64","virtualizationSystem":"kvm","virtualizationRole":"host","hostId":"d29f247f-157d-4018-b712-63fad7198028"}
I1009 09:02:41.000152   14250 start.go:138] virtualization: kvm host
I1009 09:02:41.067585   14250 out.go:177] üòÑ  minikube v1.31.2 on Ubuntu 22.04
I1009 09:02:41.196355   14250 notify.go:220] Checking for updates...
I1009 09:02:41.239065   14250 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1009 09:02:41.239634   14250 driver.go:373] Setting default libvirt URI to qemu:///system
I1009 09:02:42.298375   14250 docker.go:121] docker version: linux-24.0.6:Docker Engine - Community
I1009 09:02:42.298469   14250 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1009 09:02:48.682261   14250 cli_runner.go:217] Completed: docker system info --format "{{json .}}": (6.383762753s)
I1009 09:02:48.682625   14250 info.go:266] docker info: {ID:a2200fa7-3f9f-4282-84f8-b7d921fd3801 Containers:5 ContainersRunning:0 ContainersPaused:0 ContainersStopped:5 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:false NGoroutines:34 SystemTime:2023-10-09 09:02:48.672895354 +0300 EAT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.2.0-34-generic OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:12408430592 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:spartan-HP-ProBook-450-G1 Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0]] Warnings:<nil>}}
I1009 09:02:48.682708   14250 docker.go:294] overlay module found
I1009 09:02:48.734878   14250 out.go:177] ‚ú®  Using the docker driver based on existing profile
I1009 09:02:48.768322   14250 start.go:298] selected driver: docker
I1009 09:02:48.768331   14250 start.go:902] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/spartan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1009 09:02:48.768450   14250 start.go:913] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I1009 09:02:48.768555   14250 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I1009 09:02:48.952880   14250 info.go:266] docker info: {ID:a2200fa7-3f9f-4282-84f8-b7d921fd3801 Containers:5 ContainersRunning:0 ContainersPaused:0 ContainersStopped:5 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:false CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:22 OomKillDisable:false NGoroutines:34 SystemTime:2023-10-09 09:02:48.944268824 +0300 EAT LoggingDriver:json-file CgroupDriver:systemd NEventsListener:0 KernelVersion:6.2.0-34-generic OperatingSystem:Ubuntu 22.04.3 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:4 MemTotal:12408430592 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:spartan-HP-ProBook-450-G1 Labels:[] ExperimentalBuild:false ServerVersion:24.0.6 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523 Expected:61f9fd88f79f081d64d6fa3bb1a0dc71ec870523} RuncCommit:{ID:v1.1.9-0-gccaecfc Expected:v1.1.9-0-gccaecfc} InitCommit:{ID:de40ad0 Expected:de40ad0} SecurityOptions:[name=apparmor name=seccomp,profile=builtin name=cgroupns] ProductLicense: Warnings:<nil> ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.11.2] map[Name:compose Path:/usr/libexec/docker/cli-plugins/docker-compose SchemaVersion:0.1.0 ShortDescription:Docker Compose Vendor:Docker Inc. Version:v2.21.0]] Warnings:<nil>}}
I1009 09:02:48.953474   14250 cni.go:84] Creating CNI manager for ""
I1009 09:02:48.953541   14250 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1009 09:02:48.953553   14250 start_flags.go:319] config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/spartan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1009 09:02:49.024117   14250 out.go:177] üëç  Starting control plane node minikube in cluster minikube
I1009 09:02:49.057527   14250 cache.go:122] Beginning downloading kic base image for docker with docker
I1009 09:02:49.102115   14250 out.go:177] üöú  Pulling base image ...
I1009 09:02:49.135641   14250 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1009 09:02:49.135687   14250 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I1009 09:02:49.135775   14250 preload.go:148] Found local preload: /home/spartan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4
I1009 09:02:49.135784   14250 cache.go:57] Caching tarball of preloaded images
I1009 09:02:49.135948   14250 preload.go:174] Found /home/spartan/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.27.4-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I1009 09:02:49.135963   14250 cache.go:60] Finished verifying existence of preloaded tar for  v1.27.4 on docker
I1009 09:02:49.136083   14250 profile.go:148] Saving config to /home/spartan/.minikube/profiles/minikube/config.json ...
I1009 09:02:49.152964   14250 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 to local cache
I1009 09:02:49.153111   14250 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory
I1009 09:02:49.167759   14250 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory, skipping pull
I1009 09:02:49.167769   14250 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in cache, skipping pull
I1009 09:02:49.167783   14250 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 as a tarball
I1009 09:02:49.167788   14250 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 from local cache
I1009 09:02:49.190938   14250 cache.go:169] failed to download gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631, will try fallback image if available: tarball: unexpected EOF
I1009 09:02:49.190972   14250 image.go:79] Checking for docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local docker daemon
I1009 09:02:49.231239   14250 cache.go:150] Downloading docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 to local cache
I1009 09:02:49.231380   14250 image.go:63] Checking for docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory
I1009 09:02:49.231408   14250 image.go:66] Found docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 in local cache directory, skipping pull
I1009 09:02:49.231417   14250 image.go:105] docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 exists in cache, skipping pull
I1009 09:02:49.231426   14250 cache.go:153] successfully saved docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 as a tarball
I1009 09:02:49.231430   14250 cache.go:163] Loading docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 from local cache
I1009 09:02:49.251871   14250 cache.go:169] failed to download docker.io/kicbase/stable:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631, will try fallback image if available: tarball: unexpected EOF
I1009 09:02:49.251886   14250 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40 in local docker daemon
I1009 09:02:49.269828   14250 cache.go:150] Downloading gcr.io/k8s-minikube/kicbase:v0.0.40 to local cache
I1009 09:02:49.269992   14250 image.go:63] Checking for gcr.io/k8s-minikube/kicbase:v0.0.40 in local cache directory
I1009 09:02:49.270020   14250 image.go:66] Found gcr.io/k8s-minikube/kicbase:v0.0.40 in local cache directory, skipping pull
I1009 09:02:49.270024   14250 image.go:105] gcr.io/k8s-minikube/kicbase:v0.0.40 exists in cache, skipping pull
I1009 09:02:49.270033   14250 cache.go:153] successfully saved gcr.io/k8s-minikube/kicbase:v0.0.40 as a tarball
I1009 09:02:49.270048   14250 cache.go:163] Loading gcr.io/k8s-minikube/kicbase:v0.0.40 from local cache
I1009 09:02:49.314057   14250 cache.go:169] failed to download gcr.io/k8s-minikube/kicbase:v0.0.40, will try fallback image if available: tarball: unexpected EOF
I1009 09:02:49.314071   14250 image.go:79] Checking for docker.io/kicbase/stable:v0.0.40 in local docker daemon
I1009 09:02:49.331970   14250 cache.go:150] Downloading docker.io/kicbase/stable:v0.0.40 to local cache
I1009 09:02:49.332169   14250 image.go:63] Checking for docker.io/kicbase/stable:v0.0.40 in local cache directory
I1009 09:02:49.332201   14250 image.go:66] Found docker.io/kicbase/stable:v0.0.40 in local cache directory, skipping pull
I1009 09:02:49.332216   14250 image.go:105] docker.io/kicbase/stable:v0.0.40 exists in cache, skipping pull
I1009 09:02:49.332228   14250 cache.go:153] successfully saved docker.io/kicbase/stable:v0.0.40 as a tarball
I1009 09:02:49.332234   14250 cache.go:163] Loading docker.io/kicbase/stable:v0.0.40 from local cache
I1009 09:02:49.351550   14250 cache.go:169] failed to download docker.io/kicbase/stable:v0.0.40, will try fallback image if available: tarball: unexpected EOF
E1009 09:02:49.351572   14250 cache.go:190] Error downloading kic artifacts:  failed to download kic base image or any fallback image
I1009 09:02:49.351591   14250 cache.go:195] Successfully downloaded all kic artifacts
I1009 09:02:49.351643   14250 start.go:365] acquiring machines lock for minikube: {Name:mk6f0b49841a0cce69b88cb32d226b08db9ae809 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I1009 09:02:49.351747   14250 start.go:369] acquired machines lock for "minikube" in 78.58¬µs
I1009 09:02:49.368538   14250 start.go:96] Skipping create...Using existing machine configuration
I1009 09:02:49.368555   14250 fix.go:54] fixHost starting: 
I1009 09:02:49.368871   14250 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1009 09:02:49.427054   14250 fix.go:102] recreateIfNeeded on minikube: state=Stopped err=<nil>
W1009 09:02:49.427083   14250 fix.go:128] unexpected machine state, will restart: <nil>
I1009 09:02:49.516351   14250 out.go:177] üîÑ  Restarting existing docker container for "minikube" ...
I1009 09:02:49.560965   14250 cli_runner.go:164] Run: docker start minikube
I1009 09:02:52.300514   14250 cli_runner.go:217] Completed: docker start minikube: (2.739518596s)
I1009 09:02:52.300600   14250 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1009 09:02:52.316135   14250 kic.go:426] container "minikube" state is running.
I1009 09:02:52.316534   14250 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1009 09:02:52.331914   14250 profile.go:148] Saving config to /home/spartan/.minikube/profiles/minikube/config.json ...
I1009 09:02:52.332163   14250 machine.go:88] provisioning docker machine ...
I1009 09:02:52.332179   14250 ubuntu.go:169] provisioning hostname "minikube"
I1009 09:02:52.332264   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:02:52.368525   14250 main.go:141] libmachine: Using SSH client type: native
I1009 09:02:52.406981   14250 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1009 09:02:52.406996   14250 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I1009 09:02:52.427799   14250 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:52264->127.0.0.1:32772: read: connection reset by peer
I1009 09:02:55.428819   14250 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:52266->127.0.0.1:32772: read: connection reset by peer
I1009 09:02:58.431263   14250 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:52278->127.0.0.1:32772: read: connection reset by peer
I1009 09:03:01.432873   14250 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:48678->127.0.0.1:32772: read: connection reset by peer
I1009 09:03:04.433761   14250 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:48686->127.0.0.1:32772: read: connection reset by peer
I1009 09:03:10.849884   14250 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I1009 09:03:10.849978   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:03:10.866836   14250 main.go:141] libmachine: Using SSH client type: native
I1009 09:03:10.867343   14250 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1009 09:03:10.867357   14250 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I1009 09:03:11.000298   14250 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1009 09:03:11.032294   14250 ubuntu.go:175] set auth options {CertDir:/home/spartan/.minikube CaCertPath:/home/spartan/.minikube/certs/ca.pem CaPrivateKeyPath:/home/spartan/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/spartan/.minikube/machines/server.pem ServerKeyPath:/home/spartan/.minikube/machines/server-key.pem ClientKeyPath:/home/spartan/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/spartan/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/spartan/.minikube}
I1009 09:03:11.032326   14250 ubuntu.go:177] setting up certificates
I1009 09:03:11.032332   14250 provision.go:83] configureAuth start
I1009 09:03:11.032388   14250 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1009 09:03:11.047804   14250 provision.go:138] copyHostCerts
I1009 09:03:11.131575   14250 exec_runner.go:144] found /home/spartan/.minikube/ca.pem, removing ...
I1009 09:03:11.131590   14250 exec_runner.go:203] rm: /home/spartan/.minikube/ca.pem
I1009 09:03:11.131665   14250 exec_runner.go:151] cp: /home/spartan/.minikube/certs/ca.pem --> /home/spartan/.minikube/ca.pem (1078 bytes)
I1009 09:03:11.146587   14250 exec_runner.go:144] found /home/spartan/.minikube/cert.pem, removing ...
I1009 09:03:11.146596   14250 exec_runner.go:203] rm: /home/spartan/.minikube/cert.pem
I1009 09:03:11.146640   14250 exec_runner.go:151] cp: /home/spartan/.minikube/certs/cert.pem --> /home/spartan/.minikube/cert.pem (1123 bytes)
I1009 09:03:11.147245   14250 exec_runner.go:144] found /home/spartan/.minikube/key.pem, removing ...
I1009 09:03:11.147251   14250 exec_runner.go:203] rm: /home/spartan/.minikube/key.pem
I1009 09:03:11.147283   14250 exec_runner.go:151] cp: /home/spartan/.minikube/certs/key.pem --> /home/spartan/.minikube/key.pem (1675 bytes)
I1009 09:03:11.147473   14250 provision.go:112] generating server cert: /home/spartan/.minikube/machines/server.pem ca-key=/home/spartan/.minikube/certs/ca.pem private-key=/home/spartan/.minikube/certs/ca-key.pem org=spartan.minikube san=[192.168.58.2 127.0.0.1 localhost 127.0.0.1 minikube minikube]
I1009 09:03:11.459721   14250 provision.go:172] copyRemoteCerts
I1009 09:03:11.459802   14250 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I1009 09:03:11.459840   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:03:11.475366   14250 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/spartan/.minikube/machines/minikube/id_rsa Username:docker}
I1009 09:03:11.570594   14250 ssh_runner.go:362] scp /home/spartan/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1078 bytes)
I1009 09:03:11.774052   14250 ssh_runner.go:362] scp /home/spartan/.minikube/machines/server.pem --> /etc/docker/server.pem (1204 bytes)
I1009 09:03:11.865673   14250 ssh_runner.go:362] scp /home/spartan/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1675 bytes)
I1009 09:03:11.981317   14250 provision.go:86] duration metric: configureAuth took 948.972664ms
I1009 09:03:11.981333   14250 ubuntu.go:193] setting minikube options for container-runtime
I1009 09:03:11.981493   14250 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1009 09:03:11.981537   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:03:11.997835   14250 main.go:141] libmachine: Using SSH client type: native
I1009 09:03:11.998601   14250 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1009 09:03:11.998618   14250 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I1009 09:03:12.182793   14250 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I1009 09:03:12.182804   14250 ubuntu.go:71] root file system type: overlay
I1009 09:03:12.182968   14250 provision.go:309] Updating docker unit: /lib/systemd/system/docker.service ...
I1009 09:03:12.183035   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:03:12.199405   14250 main.go:141] libmachine: Using SSH client type: native
I1009 09:03:12.199866   14250 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1009 09:03:12.199930   14250 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I1009 09:03:12.342281   14250 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I1009 09:03:12.342370   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:03:12.359088   14250 main.go:141] libmachine: Using SSH client type: native
I1009 09:03:12.359564   14250 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x80f160] 0x812200 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I1009 09:03:12.359581   14250 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I1009 09:03:12.641406   14250 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I1009 09:03:12.641421   14250 machine.go:91] provisioned docker machine in 20.309250213s
I1009 09:03:12.641429   14250 start.go:300] post-start starting for "minikube" (driver="docker")
I1009 09:03:12.641438   14250 start.go:329] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I1009 09:03:12.641510   14250 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I1009 09:03:12.641563   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:03:12.657850   14250 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/spartan/.minikube/machines/minikube/id_rsa Username:docker}
I1009 09:03:12.814924   14250 ssh_runner.go:195] Run: cat /etc/os-release
I1009 09:03:12.818547   14250 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I1009 09:03:12.818583   14250 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I1009 09:03:12.818593   14250 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I1009 09:03:12.818598   14250 info.go:137] Remote host: Ubuntu 22.04.2 LTS
I1009 09:03:12.818605   14250 filesync.go:126] Scanning /home/spartan/.minikube/addons for local assets ...
I1009 09:03:12.890210   14250 filesync.go:126] Scanning /home/spartan/.minikube/files for local assets ...
I1009 09:03:12.890421   14250 start.go:303] post-start completed in 248.983087ms
I1009 09:03:12.890491   14250 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I1009 09:03:12.890533   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:03:12.906072   14250 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/spartan/.minikube/machines/minikube/id_rsa Username:docker}
I1009 09:03:13.051391   14250 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I1009 09:03:13.055991   14250 fix.go:56] fixHost completed within 23.687435339s
I1009 09:03:13.056006   14250 start.go:83] releasing machines lock for "minikube", held for 23.70424923s
I1009 09:03:13.056068   14250 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I1009 09:03:13.071689   14250 ssh_runner.go:195] Run: cat /version.json
I1009 09:03:13.071739   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:03:13.071746   14250 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I1009 09:03:13.071804   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:03:13.089730   14250 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/spartan/.minikube/machines/minikube/id_rsa Username:docker}
I1009 09:03:13.090502   14250 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/spartan/.minikube/machines/minikube/id_rsa Username:docker}
I1009 09:03:15.524983   14250 ssh_runner.go:235] Completed: cat /version.json: (2.453272802s)
I1009 09:03:15.525002   14250 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (2.453235491s)
I1009 09:03:15.525115   14250 ssh_runner.go:195] Run: systemctl --version
I1009 09:03:15.612986   14250 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I1009 09:03:15.617630   14250 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I1009 09:03:15.718018   14250 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I1009 09:03:15.718100   14250 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I1009 09:03:15.727904   14250 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I1009 09:03:15.727922   14250 start.go:466] detecting cgroup driver to use...
I1009 09:03:15.727949   14250 detect.go:199] detected "systemd" cgroup driver on host os
I1009 09:03:15.728056   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I1009 09:03:15.747080   14250 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I1009 09:03:15.787107   14250 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I1009 09:03:15.798632   14250 containerd.go:145] configuring containerd to use "systemd" as cgroup driver...
I1009 09:03:15.798706   14250 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = true|g' /etc/containerd/config.toml"
I1009 09:03:15.810060   14250 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1009 09:03:15.821291   14250 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I1009 09:03:15.833331   14250 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I1009 09:03:15.849625   14250 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I1009 09:03:15.864494   14250 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I1009 09:03:15.876768   14250 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I1009 09:03:15.946890   14250 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I1009 09:03:16.083141   14250 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1009 09:03:16.267002   14250 ssh_runner.go:195] Run: sudo systemctl restart containerd
I1009 09:03:16.414274   14250 start.go:466] detecting cgroup driver to use...
I1009 09:03:16.414311   14250 detect.go:199] detected "systemd" cgroup driver on host os
I1009 09:03:16.414365   14250 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I1009 09:03:16.428522   14250 cruntime.go:276] skipping containerd shutdown because we are bound to it
I1009 09:03:16.428577   14250 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I1009 09:03:16.621745   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I1009 09:03:16.640286   14250 ssh_runner.go:195] Run: which cri-dockerd
I1009 09:03:16.643700   14250 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I1009 09:03:16.653230   14250 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I1009 09:03:16.672612   14250 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I1009 09:03:17.110662   14250 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I1009 09:03:17.231182   14250 docker.go:535] configuring docker to use "systemd" as cgroup driver...
I1009 09:03:17.231204   14250 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (143 bytes)
I1009 09:03:17.251060   14250 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1009 09:03:17.350838   14250 ssh_runner.go:195] Run: sudo systemctl restart docker
I1009 09:03:26.323916   14250 ssh_runner.go:235] Completed: sudo systemctl restart docker: (8.973053098s)
I1009 09:03:26.323986   14250 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1009 09:03:26.503291   14250 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I1009 09:03:26.634798   14250 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I1009 09:03:26.742891   14250 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1009 09:03:26.862413   14250 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I1009 09:03:26.893183   14250 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I1009 09:03:27.024165   14250 ssh_runner.go:195] Run: sudo systemctl restart cri-docker
I1009 09:03:29.290997   14250 ssh_runner.go:235] Completed: sudo systemctl restart cri-docker: (2.266807642s)
I1009 09:03:29.291011   14250 start.go:513] Will wait 60s for socket path /var/run/cri-dockerd.sock
I1009 09:03:29.291102   14250 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I1009 09:03:29.295011   14250 start.go:534] Will wait 60s for crictl version
I1009 09:03:29.295066   14250 ssh_runner.go:195] Run: which crictl
I1009 09:03:29.298505   14250 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I1009 09:03:30.600548   14250 ssh_runner.go:235] Completed: sudo /usr/bin/crictl version: (1.3020195s)
I1009 09:03:30.600560   14250 start.go:550] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  24.0.4
RuntimeApiVersion:  v1
I1009 09:03:30.600602   14250 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1009 09:03:31.578624   14250 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I1009 09:03:31.831225   14250 out.go:204] üê≥  Preparing Kubernetes v1.27.4 on Docker 24.0.4 ...
I1009 09:03:31.842270   14250 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I1009 09:03:31.857816   14250 ssh_runner.go:195] Run: grep 192.168.58.1	host.minikube.internal$ /etc/hosts
I1009 09:03:31.862070   14250 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.58.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1009 09:03:31.875971   14250 preload.go:132] Checking if preload exists for k8s version v1.27.4 and runtime docker
I1009 09:03:31.876034   14250 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1009 09:03:31.918183   14250 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
mongo-express:latest
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1009 09:03:31.918197   14250 docker.go:566] Images already preloaded, skipping extraction
I1009 09:03:31.918268   14250 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I1009 09:03:31.936281   14250 docker.go:636] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.27.4
registry.k8s.io/kube-controller-manager:v1.27.4
registry.k8s.io/kube-scheduler:v1.27.4
registry.k8s.io/kube-proxy:v1.27.4
registry.k8s.io/coredns/coredns:v1.10.1
registry.k8s.io/etcd:3.5.7-0
registry.k8s.io/pause:3.9
mongo-express:latest
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I1009 09:03:31.936292   14250 cache_images.go:84] Images are preloaded, skipping loading
I1009 09:03:31.936347   14250 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I1009 09:03:33.804442   14250 ssh_runner.go:235] Completed: docker info --format {{.CgroupDriver}}: (1.868071725s)
I1009 09:03:33.804493   14250 cni.go:84] Creating CNI manager for ""
I1009 09:03:33.804510   14250 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1009 09:03:33.804526   14250 kubeadm.go:87] Using pod CIDR: 10.244.0.0/16
I1009 09:03:33.804542   14250 kubeadm.go:176] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.58.2 APIServerPort:8443 KubernetesVersion:v1.27.4 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.58.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.58.2 CgroupDriver:systemd ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I1009 09:03:33.804673   14250 kubeadm.go:181] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.58.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.58.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.58.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.27.4
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: systemd
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I1009 09:03:33.804747   14250 kubeadm.go:976] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.27.4/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --container-runtime-endpoint=unix:///var/run/cri-dockerd.sock --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.58.2

[Install]
 config:
{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:}
I1009 09:03:33.804801   14250 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.27.4
I1009 09:03:33.851173   14250 binaries.go:44] Found k8s binaries, skipping transfer
I1009 09:03:33.851242   14250 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I1009 09:03:33.860743   14250 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (369 bytes)
I1009 09:03:33.880909   14250 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I1009 09:03:33.900931   14250 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2090 bytes)
I1009 09:03:33.934195   14250 ssh_runner.go:195] Run: grep 192.168.58.2	control-plane.minikube.internal$ /etc/hosts
I1009 09:03:33.937605   14250 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.58.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I1009 09:03:33.949758   14250 certs.go:56] Setting up /home/spartan/.minikube/profiles/minikube for IP: 192.168.58.2
I1009 09:03:33.949787   14250 certs.go:190] acquiring lock for shared ca certs: {Name:mka978804772115f37e9517859f63c227c116ed0 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1009 09:03:33.974273   14250 certs.go:199] skipping minikubeCA CA generation: /home/spartan/.minikube/ca.key
I1009 09:03:33.974505   14250 certs.go:199] skipping proxyClientCA CA generation: /home/spartan/.minikube/proxy-client-ca.key
I1009 09:03:33.974731   14250 certs.go:315] skipping minikube-user signed cert generation: /home/spartan/.minikube/profiles/minikube/client.key
I1009 09:03:33.974940   14250 certs.go:315] skipping minikube signed cert generation: /home/spartan/.minikube/profiles/minikube/apiserver.key.cee25041
I1009 09:03:33.975132   14250 certs.go:315] skipping aggregator signed cert generation: /home/spartan/.minikube/profiles/minikube/proxy-client.key
I1009 09:03:33.975279   14250 certs.go:437] found cert: /home/spartan/.minikube/certs/home/spartan/.minikube/certs/ca-key.pem (1675 bytes)
I1009 09:03:33.975326   14250 certs.go:437] found cert: /home/spartan/.minikube/certs/home/spartan/.minikube/certs/ca.pem (1078 bytes)
I1009 09:03:33.975370   14250 certs.go:437] found cert: /home/spartan/.minikube/certs/home/spartan/.minikube/certs/cert.pem (1123 bytes)
I1009 09:03:33.975421   14250 certs.go:437] found cert: /home/spartan/.minikube/certs/home/spartan/.minikube/certs/key.pem (1675 bytes)
I1009 09:03:33.976513   14250 ssh_runner.go:362] scp /home/spartan/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1399 bytes)
I1009 09:03:34.005513   14250 ssh_runner.go:362] scp /home/spartan/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1679 bytes)
I1009 09:03:34.035074   14250 ssh_runner.go:362] scp /home/spartan/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I1009 09:03:34.108898   14250 ssh_runner.go:362] scp /home/spartan/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1675 bytes)
I1009 09:03:34.141847   14250 ssh_runner.go:362] scp /home/spartan/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I1009 09:03:34.171100   14250 ssh_runner.go:362] scp /home/spartan/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1675 bytes)
I1009 09:03:34.204170   14250 ssh_runner.go:362] scp /home/spartan/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I1009 09:03:34.233113   14250 ssh_runner.go:362] scp /home/spartan/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1679 bytes)
I1009 09:03:34.259951   14250 ssh_runner.go:362] scp /home/spartan/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I1009 09:03:34.289764   14250 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I1009 09:03:34.310075   14250 ssh_runner.go:195] Run: openssl version
I1009 09:03:34.368610   14250 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I1009 09:03:34.412410   14250 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I1009 09:03:34.416327   14250 certs.go:480] hashing: -rw-r--r-- 1 root root 1111 Sep  2 15:08 /usr/share/ca-certificates/minikubeCA.pem
I1009 09:03:34.416371   14250 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I1009 09:03:34.423410   14250 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I1009 09:03:34.432930   14250 ssh_runner.go:195] Run: ls /var/lib/minikube/certs/etcd
I1009 09:03:34.448080   14250 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I1009 09:03:34.460563   14250 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I1009 09:03:34.479056   14250 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I1009 09:03:34.486585   14250 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I1009 09:03:34.493499   14250 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I1009 09:03:34.500611   14250 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I1009 09:03:34.507729   14250 kubeadm.go:404] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.40@sha256:8cadf23777709e43eca447c47a45f5a4635615129267ce025193040ec92a1631 Memory:2900 CPUs:2 DiskSize:20000 VMDriver: Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:0 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.27.4 ClusterName:minikube Namespace:default APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI: NodeIP: NodePort:8443 NodeName:} Nodes:[{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/spartan:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0}
I1009 09:03:34.507840   14250 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1009 09:03:34.547999   14250 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
I1009 09:03:34.581189   14250 kubeadm.go:419] found existing configuration files, will attempt cluster restart
I1009 09:03:34.581212   14250 kubeadm.go:636] restartCluster start
I1009 09:03:34.581274   14250 ssh_runner.go:195] Run: sudo test -d /data/minikube
I1009 09:03:34.623432   14250 kubeadm.go:127] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I1009 09:03:34.654700   14250 kubeconfig.go:135] verify returned: extract IP: "minikube" does not appear in /home/spartan/.kube/config
I1009 09:03:34.654800   14250 kubeconfig.go:146] "minikube" context is missing from /home/spartan/.kube/config - will repair!
I1009 09:03:34.674423   14250 lock.go:35] WriteFile acquiring /home/spartan/.kube/config: {Name:mka8d2c79140dabfaab214566cd7ef92a8717895 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1009 09:03:34.748968   14250 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I1009 09:03:34.793358   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:34.793431   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:34.861519   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:34.861530   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:34.861583   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:34.872489   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:35.372904   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:35.372961   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:35.385154   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:35.872794   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:35.872854   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:35.885213   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:36.373043   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:36.373103   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:36.385254   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:36.872785   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:36.872846   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:36.884757   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:37.373324   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:37.373391   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:37.385481   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:37.873299   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:37.873358   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:37.885252   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:38.372867   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:38.372955   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:38.385283   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:38.872823   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:38.872899   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:38.885199   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:39.372554   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:39.372620   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:39.384543   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:39.872904   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:39.872983   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:39.884696   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:40.373051   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:40.373113   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:40.384986   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:40.873333   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:40.873422   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:40.885305   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:41.372914   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:41.372985   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:41.384766   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:41.873377   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:41.873447   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:41.884749   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:42.373202   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:42.373288   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:42.384892   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:42.873350   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:42.873420   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:42.885860   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:43.373505   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:43.373585   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:43.386078   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:43.872953   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:43.873036   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:43.884072   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:44.373014   14250 api_server.go:166] Checking apiserver status ...
I1009 09:03:44.373085   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W1009 09:03:44.385243   14250 api_server.go:170] stopped: unable to get apiserver pid: sudo pgrep -xnf kube-apiserver.*minikube.*: Process exited with status 1
stdout:

stderr:
I1009 09:03:44.793990   14250 kubeadm.go:611] needs reconfigure: apiserver error: context deadline exceeded
I1009 09:03:44.794055   14250 kubeadm.go:1128] stopping kube-system containers ...
I1009 09:03:44.794117   14250 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I1009 09:03:44.814408   14250 docker.go:462] Stopping containers: [e86cf55705a7 f9d813024c1b b6664c78c262 43db99ca2815 dfd60e69efbd f0fbbea2f3fe 5f40b948d47d a099c60c8a87 7c548f5602c2 2a7f2491e1e4 eac185715e7f b22e3e13c929 016a556e2057 89955883f3b2 33ef5d1581d0 282b7e1c61d9 2331be1d8063 a32b3565bf21 b974bb77977e bf790da0a2c4 e31d4899c59b 1cc64c61d38f 10cd97a6d214 905eaeeceeea 920e67bde6a0]
I1009 09:03:44.814478   14250 ssh_runner.go:195] Run: docker stop e86cf55705a7 f9d813024c1b b6664c78c262 43db99ca2815 dfd60e69efbd f0fbbea2f3fe 5f40b948d47d a099c60c8a87 7c548f5602c2 2a7f2491e1e4 eac185715e7f b22e3e13c929 016a556e2057 89955883f3b2 33ef5d1581d0 282b7e1c61d9 2331be1d8063 a32b3565bf21 b974bb77977e bf790da0a2c4 e31d4899c59b 1cc64c61d38f 10cd97a6d214 905eaeeceeea 920e67bde6a0
I1009 09:03:44.853764   14250 ssh_runner.go:195] Run: sudo systemctl stop kubelet
I1009 09:03:44.867122   14250 ssh_runner.go:195] Run: sudo ls -la /etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf
I1009 09:03:44.877060   14250 kubeadm.go:155] found existing configuration files:
-rw------- 1 root root 5643 Sep  2 15:08 /etc/kubernetes/admin.conf
-rw------- 1 root root 5656 Sep  4 09:22 /etc/kubernetes/controller-manager.conf
-rw------- 1 root root 1971 Sep  2 15:09 /etc/kubernetes/kubelet.conf
-rw------- 1 root root 5600 Sep  4 09:22 /etc/kubernetes/scheduler.conf

I1009 09:03:44.877117   14250 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/admin.conf
I1009 09:03:44.905958   14250 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/kubelet.conf
I1009 09:03:44.924199   14250 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf
I1009 09:03:44.966364   14250 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/controller-manager.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/controller-manager.conf: Process exited with status 1
stdout:

stderr:
I1009 09:03:44.966423   14250 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/controller-manager.conf
I1009 09:03:44.975731   14250 ssh_runner.go:195] Run: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf
I1009 09:03:44.985701   14250 kubeadm.go:166] "https://control-plane.minikube.internal:8443" may not be in /etc/kubernetes/scheduler.conf - will remove: sudo grep https://control-plane.minikube.internal:8443 /etc/kubernetes/scheduler.conf: Process exited with status 1
stdout:

stderr:
I1009 09:03:44.985762   14250 ssh_runner.go:195] Run: sudo rm -f /etc/kubernetes/scheduler.conf
I1009 09:03:44.995336   14250 ssh_runner.go:195] Run: sudo cp /var/tmp/minikube/kubeadm.yaml.new /var/tmp/minikube/kubeadm.yaml
I1009 09:03:45.005101   14250 kubeadm.go:713] reconfiguring cluster from /var/tmp/minikube/kubeadm.yaml
I1009 09:03:45.005131   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml"
I1009 09:03:47.276687   14250 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase certs all --config /var/tmp/minikube/kubeadm.yaml": (2.271534132s)
I1009 09:03:47.276709   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubeconfig all --config /var/tmp/minikube/kubeadm.yaml"
I1009 09:03:48.229020   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase kubelet-start --config /var/tmp/minikube/kubeadm.yaml"
I1009 09:03:48.456838   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase control-plane all --config /var/tmp/minikube/kubeadm.yaml"
I1009 09:03:48.607250   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase etcd local --config /var/tmp/minikube/kubeadm.yaml"
I1009 09:03:48.663335   14250 api_server.go:52] waiting for apiserver process to appear ...
I1009 09:03:48.663419   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:48.675187   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:49.187345   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:49.687701   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:50.187307   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:50.687265   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:51.187382   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:51.687606   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:52.187838   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:52.687060   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:53.187163   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:53.686984   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:54.187161   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:54.687522   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:55.187803   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:55.687778   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:56.187811   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:56.687805   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:57.187038   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:57.687551   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:58.187552   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:58.687170   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:59.187332   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:03:59.686784   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:00.187389   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:00.687220   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:01.187796   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:01.687848   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:02.186885   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:02.687545   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:03.186949   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:03.687227   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:04.187505   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:04.687135   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:05.187297   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:05.686851   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:06.186936   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:06.687721   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:07.187179   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:07.687772   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:08.187003   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:08.687753   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:09.187185   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:09.687637   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:10.187029   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:10.686912   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:11.187512   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:11.686968   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:12.187710   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:12.687901   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:13.187084   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:13.687614   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:14.187163   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:14.687774   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:15.186815   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:15.687542   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:04:15.699278   14250 api_server.go:72] duration metric: took 27.035940339s to wait for apiserver process to appear ...
I1009 09:04:15.699291   14250 api_server.go:88] waiting for apiserver healthz status ...
I1009 09:04:15.699304   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:15.699556   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:15.699578   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:15.699757   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:16.200653   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:16.551695   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:16.700046   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:16.700365   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:17.201741   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:17.202306   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:17.700131   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:17.700493   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:18.200226   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:18.200561   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:18.700074   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:18.700348   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:19.200143   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:19.201100   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:19.700262   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:19.700612   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:20.200251   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:20.200596   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:20.700181   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:20.700580   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:21.200197   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:21.200547   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:21.700398   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:21.700732   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:22.200255   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:22.200572   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:22.700365   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:22.700849   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:23.200573   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:23.201093   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:23.700760   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:23.701159   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:24.200710   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:24.201111   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:24.700152   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:24.700652   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:25.200409   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:25.200824   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:25.700026   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:25.700478   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:26.200184   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:26.200641   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:26.700459   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:26.700784   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:27.200359   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:27.212352   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:27.700336   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:27.700778   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:28.200282   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:28.200693   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:28.700533   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:28.700923   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:29.199975   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:29.200434   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:29.701008   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:29.701371   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:30.200344   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:30.200836   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:30.700471   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:30.700891   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:31.199821   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:31.200149   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:31.700520   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:31.700880   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:32.200556   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:32.201569   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:32.699952   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:32.700350   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:33.200804   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:33.201154   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:33.700162   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:33.700496   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:34.200092   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:34.200436   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:34.700329   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:34.701141   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:35.199822   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:35.200121   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:35.700575   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:35.700907   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:36.200560   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:36.200945   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:36.700364   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:36.700657   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:37.200130   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:37.200562   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:37.700544   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:37.700864   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:38.200455   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:38.200812   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:38.700305   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:38.700640   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:39.200353   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:39.200914   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:39.700021   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:39.700391   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:40.200253   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:40.200602   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:40.700370   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:40.700698   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:41.200551   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:41.200931   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:41.700401   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:41.700785   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:42.200556   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:42.200905   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:42.700510   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:42.700845   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:43.200835   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:43.201164   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:43.699883   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:43.700233   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:44.200818   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:44.201177   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:44.700821   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:44.701211   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:45.200546   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:45.200827   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:45.700262   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:45.700616   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:46.200736   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:46.201088   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:46.700703   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:46.701099   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:47.200906   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:47.201705   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:47.700566   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:47.700988   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:48.200088   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:48.200945   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:48.700839   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:48.701255   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:49.200052   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:49.200405   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:49.700684   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:49.701844   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:50.200187   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:50.200510   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:50.700514   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:50.700833   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:51.200631   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:51.264486   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:51.700537   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:51.701450   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:52.200201   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:52.200553   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:52.700097   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:52.700447   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:53.200598   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:53.201173   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:53.700159   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:53.700553   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:54.200333   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:54.200647   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:54.700571   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:54.700868   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:55.200667   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:55.200982   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:55.700516   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:55.700850   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:56.200761   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:56.201658   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:56.700582   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:56.700905   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:57.200192   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:57.200535   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:57.700740   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:57.701077   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:58.200249   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:58.200561   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:58.700523   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:58.700881   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:59.200235   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:59.200550   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:04:59.700634   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:04:59.701506   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:05:00.200787   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:00.201118   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:05:00.700395   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:00.700734   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:05:01.199996   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:01.212650   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:05:01.700547   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:01.701331   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:05:02.199817   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:02.200122   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:05:02.700305   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:02.700594   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": dial tcp 192.168.58.2:8443: connect: connection refused
I1009 09:05:03.200045   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:08.201053   14250 api_server.go:269] stopped: https://192.168.58.2:8443/healthz: Get "https://192.168.58.2:8443/healthz": context deadline exceeded (Client.Timeout exceeded while awaiting headers)
I1009 09:05:08.201082   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:08.515939   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W1009 09:05:08.515957   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I1009 09:05:08.700597   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:08.705517   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:08.705529   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:09.200090   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:09.206876   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:09.206888   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:09.700108   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:09.707055   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:09.707070   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:10.199836   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:10.206354   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:10.206371   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:10.700411   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:10.705396   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:10.705410   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:11.200399   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:11.205374   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:11.205393   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:11.700769   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:11.705788   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:11.705808   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:12.200005   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:12.204807   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:12.204819   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:12.699875   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:12.704978   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:12.704994   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:13.200501   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:13.205449   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:13.205479   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:13.700192   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:13.705117   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:13.705131   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:14.200728   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:14.205473   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:14.205485   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:14.700352   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:14.709757   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:14.709780   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:15.200852   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:15.205471   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
W1009 09:05:15.205494   14250 api_server.go:103] status: https://192.168.58.2:8443/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-kube-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[+]poststarthook/scheduling/bootstrap-system-priority-classes ok
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-deprecated-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
[+]poststarthook/apiservice-discovery-controller ok
healthz check failed
I1009 09:05:15.700798   14250 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-apiserver --format={{.ID}}
I1009 09:05:15.719192   14250 logs.go:284] 2 containers: [2145ab5d4b47 a099c60c8a87]
I1009 09:05:15.719257   14250 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_etcd --format={{.ID}}
I1009 09:05:15.736346   14250 logs.go:284] 2 containers: [df676eef581c 7c548f5602c2]
I1009 09:05:15.736426   14250 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_coredns --format={{.ID}}
I1009 09:05:15.753135   14250 logs.go:284] 1 containers: [f9d813024c1b]
I1009 09:05:15.753215   14250 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-scheduler --format={{.ID}}
I1009 09:05:15.773383   14250 logs.go:284] 2 containers: [ddeb5f0b7021 eac185715e7f]
I1009 09:05:15.773523   14250 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-proxy --format={{.ID}}
I1009 09:05:15.790870   14250 logs.go:284] 1 containers: [43db99ca2815]
I1009 09:05:15.790942   14250 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kube-controller-manager --format={{.ID}}
I1009 09:05:15.811404   14250 logs.go:284] 2 containers: [77d78ae6608a 2a7f2491e1e4]
I1009 09:05:15.811483   14250 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_kindnet --format={{.ID}}
I1009 09:05:15.828458   14250 logs.go:284] 0 containers: []
W1009 09:05:15.828471   14250 logs.go:286] No container was found matching "kindnet"
I1009 09:05:15.828519   14250 ssh_runner.go:195] Run: docker ps -a --filter=name=k8s_storage-provisioner --format={{.ID}}
I1009 09:05:15.846149   14250 logs.go:284] 1 containers: [e86cf55705a7]
I1009 09:05:15.846181   14250 logs.go:123] Gathering logs for dmesg ...
I1009 09:05:15.846193   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo dmesg -PH -L=never --level warn,err,crit,alert,emerg | tail -n 400"
I1009 09:05:16.057474   14250 logs.go:123] Gathering logs for describe nodes ...
I1009 09:05:16.105176   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig"
I1009 09:05:39.557301   14250 ssh_runner.go:235] Completed: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl describe nodes --kubeconfig=/var/lib/minikube/kubeconfig": (23.452102841s)
I1009 09:05:39.561072   14250 logs.go:123] Gathering logs for kube-apiserver [a099c60c8a87] ...
I1009 09:05:39.561082   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 a099c60c8a87"
I1009 09:05:40.147524   14250 logs.go:123] Gathering logs for etcd [7c548f5602c2] ...
I1009 09:05:40.147539   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 7c548f5602c2"
I1009 09:05:41.951504   14250 ssh_runner.go:235] Completed: /bin/bash -c "docker logs --tail 400 7c548f5602c2": (1.803946291s)
I1009 09:05:42.035075   14250 logs.go:123] Gathering logs for kube-controller-manager [77d78ae6608a] ...
I1009 09:05:42.035090   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 77d78ae6608a"
I1009 09:05:42.068578   14250 logs.go:123] Gathering logs for container status ...
I1009 09:05:42.068591   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a"
I1009 09:05:43.313656   14250 ssh_runner.go:235] Completed: /bin/bash -c "sudo `which crictl || echo crictl` ps -a || sudo docker ps -a": (1.245037164s)
I1009 09:05:43.315366   14250 logs.go:123] Gathering logs for kube-apiserver [2145ab5d4b47] ...
I1009 09:05:43.315376   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2145ab5d4b47"
I1009 09:05:43.363039   14250 logs.go:123] Gathering logs for kube-controller-manager [2a7f2491e1e4] ...
I1009 09:05:43.363054   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 2a7f2491e1e4"
I1009 09:05:44.094739   14250 logs.go:123] Gathering logs for kubelet ...
I1009 09:05:44.094751   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u kubelet -n 400"
I1009 09:05:44.246667   14250 logs.go:123] Gathering logs for etcd [df676eef581c] ...
I1009 09:05:44.246682   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 df676eef581c"
I1009 09:05:44.378115   14250 logs.go:123] Gathering logs for coredns [f9d813024c1b] ...
I1009 09:05:44.378130   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 f9d813024c1b"
I1009 09:05:44.429974   14250 logs.go:123] Gathering logs for kube-scheduler [ddeb5f0b7021] ...
I1009 09:05:44.429987   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 ddeb5f0b7021"
I1009 09:05:44.480971   14250 logs.go:123] Gathering logs for kube-scheduler [eac185715e7f] ...
I1009 09:05:44.480987   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 eac185715e7f"
I1009 09:05:44.893212   14250 logs.go:123] Gathering logs for kube-proxy [43db99ca2815] ...
I1009 09:05:44.893231   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 43db99ca2815"
I1009 09:05:44.947912   14250 logs.go:123] Gathering logs for storage-provisioner [e86cf55705a7] ...
I1009 09:05:44.947929   14250 ssh_runner.go:195] Run: /bin/bash -c "docker logs --tail 400 e86cf55705a7"
I1009 09:05:45.144427   14250 logs.go:123] Gathering logs for Docker ...
I1009 09:05:45.144445   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo journalctl -u docker -u cri-docker -n 400"
I1009 09:05:47.668051   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:48.054779   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 200:
ok
I1009 09:05:48.153868   14250 api_server.go:141] control plane version: v1.27.4
I1009 09:05:48.153886   14250 api_server.go:131] duration metric: took 1m32.454588481s to wait for apiserver health ...
I1009 09:05:48.153894   14250 cni.go:84] Creating CNI manager for ""
I1009 09:05:48.153910   14250 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I1009 09:05:48.407215   14250 out.go:177] üîó  Configuring bridge CNI (Container Networking Interface) ...
I1009 09:05:48.483971   14250 ssh_runner.go:195] Run: sudo mkdir -p /etc/cni/net.d
I1009 09:05:48.496772   14250 ssh_runner.go:362] scp memory --> /etc/cni/net.d/1-k8s.conflist (457 bytes)
I1009 09:05:48.517500   14250 system_pods.go:43] waiting for kube-system pods to appear ...
I1009 09:05:48.971400   14250 system_pods.go:59] 7 kube-system pods found
I1009 09:05:48.971419   14250 system_pods.go:61] "coredns-5d78c9869d-rcsqq" [f751b7b9-ca5e-425c-8ab6-7a8b0709d8e2] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1009 09:05:48.971426   14250 system_pods.go:61] "etcd-minikube" [5d9ee94e-efbf-417f-bc38-7c9503c0f423] Running
I1009 09:05:48.971431   14250 system_pods.go:61] "kube-apiserver-minikube" [d18c34bf-39a5-49ab-a1fc-efd383a2719f] Running
I1009 09:05:48.971435   14250 system_pods.go:61] "kube-controller-manager-minikube" [80c08c3f-2d67-4652-afcb-6c54c963643a] Running
I1009 09:05:48.971442   14250 system_pods.go:61] "kube-proxy-cgc7x" [dc481b92-7a02-4746-9212-a0f659e8ff85] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1009 09:05:48.971447   14250 system_pods.go:61] "kube-scheduler-minikube" [104ba454-0819-44f8-89b8-038cd32f2a46] Running
I1009 09:05:48.971452   14250 system_pods.go:61] "storage-provisioner" [52876b61-ea6e-45b7-996c-dd815fd75bc2] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1009 09:05:48.971457   14250 system_pods.go:74] duration metric: took 453.949009ms to wait for pod list to return data ...
I1009 09:05:48.971463   14250 node_conditions.go:102] verifying NodePressure condition ...
I1009 09:05:49.017255   14250 node_conditions.go:122] node storage ephemeral capacity is 479079112Ki
I1009 09:05:49.017293   14250 node_conditions.go:123] node cpu capacity is 4
I1009 09:05:49.017308   14250 node_conditions.go:105] duration metric: took 45.839821ms to run NodePressure ...
I1009 09:05:49.017333   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml"
I1009 09:05:53.558163   14250 ssh_runner.go:235] Completed: /bin/bash -c "sudo env PATH="/var/lib/minikube/binaries/v1.27.4:$PATH" kubeadm init phase addon all --config /var/tmp/minikube/kubeadm.yaml": (4.540800017s)
I1009 09:05:53.558181   14250 ssh_runner.go:195] Run: /bin/bash -c "cat /proc/$(pgrep kube-apiserver)/oom_adj"
I1009 09:05:53.566473   14250 ops.go:34] apiserver oom_adj: -16
I1009 09:05:53.566481   14250 kubeadm.go:640] restartCluster took 2m18.985265843s
I1009 09:05:53.566506   14250 kubeadm.go:406] StartCluster complete in 2m19.058797398s
I1009 09:05:53.566521   14250 settings.go:142] acquiring lock: {Name:mk87327fbfd1318476e933b828d9f146065c065a Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1009 09:05:53.566635   14250 settings.go:150] Updating kubeconfig:  /home/spartan/.kube/config
I1009 09:05:53.567250   14250 lock.go:35] WriteFile acquiring /home/spartan/.kube/config: {Name:mka8d2c79140dabfaab214566cd7ef92a8717895 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I1009 09:05:53.567463   14250 ssh_runner.go:195] Run: /bin/bash -c "sudo /var/lib/minikube/binaries/v1.27.4/kubectl --kubeconfig=/var/lib/minikube/kubeconfig -n kube-system get configmap coredns -o yaml"
I1009 09:05:53.567584   14250 addons.go:499] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false volumesnapshots:false]
I1009 09:05:53.567673   14250 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I1009 09:05:53.567680   14250 addons.go:69] Setting default-storageclass=true in profile "minikube"
I1009 09:05:53.567692   14250 addons.go:231] Setting addon storage-provisioner=true in "minikube"
I1009 09:05:53.567697   14250 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
W1009 09:05:53.567700   14250 addons.go:240] addon storage-provisioner should already be in state true
I1009 09:05:53.567744   14250 host.go:66] Checking if "minikube" exists ...
I1009 09:05:53.567744   14250 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.27.4
I1009 09:05:53.568098   14250 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1009 09:05:53.568119   14250 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1009 09:05:53.572620   14250 kapi.go:248] "coredns" deployment in "kube-system" namespace and "minikube" context rescaled to 1 replicas
I1009 09:05:53.572653   14250 start.go:223] Will wait 6m0s for node &{Name: IP:192.168.58.2 Port:8443 KubernetesVersion:v1.27.4 ContainerRuntime:docker ControlPlane:true Worker:true}
I1009 09:05:53.842880   14250 out.go:177] üîé  Verifying Kubernetes components...
I1009 09:05:53.600905   14250 addons.go:231] Setting addon default-storageclass=true in "minikube"
I1009 09:05:53.722997   14250 start.go:874] CoreDNS already contains "host.minikube.internal" host record, skipping...
I1009 09:05:53.942876   14250 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service kubelet
I1009 09:05:53.942881   14250 out.go:177]     ‚ñ™ Using image gcr.io/k8s-minikube/storage-provisioner:v5
W1009 09:05:53.942921   14250 addons.go:240] addon default-storageclass should already be in state true
I1009 09:05:54.011115   14250 host.go:66] Checking if "minikube" exists ...
I1009 09:05:53.957063   14250 api_server.go:52] waiting for apiserver process to appear ...
I1009 09:05:54.011193   14250 addons.go:423] installing /etc/kubernetes/addons/storage-provisioner.yaml
I1009 09:05:54.011205   14250 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I1009 09:05:54.011233   14250 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I1009 09:05:54.011261   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:05:54.011666   14250 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I1009 09:05:54.027535   14250 api_server.go:72] duration metric: took 454.84655ms to wait for apiserver process to appear ...
I1009 09:05:54.027550   14250 api_server.go:88] waiting for apiserver healthz status ...
I1009 09:05:54.027613   14250 api_server.go:253] Checking apiserver healthz at https://192.168.58.2:8443/healthz ...
I1009 09:05:54.035119   14250 addons.go:423] installing /etc/kubernetes/addons/storageclass.yaml
I1009 09:05:54.035132   14250 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I1009 09:05:54.035211   14250 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I1009 09:05:54.035849   14250 api_server.go:279] https://192.168.58.2:8443/healthz returned 200:
ok
I1009 09:05:54.036285   14250 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/spartan/.minikube/machines/minikube/id_rsa Username:docker}
I1009 09:05:54.037039   14250 api_server.go:141] control plane version: v1.27.4
I1009 09:05:54.037049   14250 api_server.go:131] duration metric: took 9.493568ms to wait for apiserver health ...
I1009 09:05:54.037055   14250 system_pods.go:43] waiting for kube-system pods to appear ...
I1009 09:05:54.055730   14250 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/spartan/.minikube/machines/minikube/id_rsa Username:docker}
I1009 09:05:54.067185   14250 system_pods.go:59] 7 kube-system pods found
I1009 09:05:54.067206   14250 system_pods.go:61] "coredns-5d78c9869d-rcsqq" [f751b7b9-ca5e-425c-8ab6-7a8b0709d8e2] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I1009 09:05:54.067216   14250 system_pods.go:61] "etcd-minikube" [5d9ee94e-efbf-417f-bc38-7c9503c0f423] Running
I1009 09:05:54.067222   14250 system_pods.go:61] "kube-apiserver-minikube" [d18c34bf-39a5-49ab-a1fc-efd383a2719f] Running
I1009 09:05:54.067229   14250 system_pods.go:61] "kube-controller-manager-minikube" [80c08c3f-2d67-4652-afcb-6c54c963643a] Running
I1009 09:05:54.067238   14250 system_pods.go:61] "kube-proxy-cgc7x" [dc481b92-7a02-4746-9212-a0f659e8ff85] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I1009 09:05:54.067245   14250 system_pods.go:61] "kube-scheduler-minikube" [104ba454-0819-44f8-89b8-038cd32f2a46] Running
I1009 09:05:54.067254   14250 system_pods.go:61] "storage-provisioner" [52876b61-ea6e-45b7-996c-dd815fd75bc2] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I1009 09:05:54.067261   14250 system_pods.go:74] duration metric: took 30.199746ms to wait for pod list to return data ...
I1009 09:05:54.067272   14250 kubeadm.go:581] duration metric: took 494.590837ms to wait for : map[apiserver:true system_pods:true] ...
I1009 09:05:54.067285   14250 node_conditions.go:102] verifying NodePressure condition ...
I1009 09:05:54.070802   14250 node_conditions.go:122] node storage ephemeral capacity is 479079112Ki
I1009 09:05:54.070820   14250 node_conditions.go:123] node cpu capacity is 4
I1009 09:05:54.070833   14250 node_conditions.go:105] duration metric: took 3.542466ms to run NodePressure ...
I1009 09:05:54.070845   14250 start.go:228] waiting for startup goroutines ...
I1009 09:05:54.301302   14250 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I1009 09:05:54.312692   14250 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I1009 09:05:57.214381   14250 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.913052294s)
I1009 09:05:57.214465   14250 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.27.4/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: (2.901752527s)
I1009 09:05:57.447973   14250 out.go:177] üåü  Enabled addons: storage-provisioner, default-storageclass
I1009 09:05:57.534053   14250 addons.go:502] enable addons completed in 3.966445488s: enabled=[storage-provisioner default-storageclass]
I1009 09:05:57.534159   14250 start.go:233] waiting for cluster config update ...
I1009 09:05:57.534188   14250 start.go:242] writing updated cluster config ...
I1009 09:05:57.534852   14250 ssh_runner.go:195] Run: rm -f paused
I1009 09:06:25.852500   14250 start.go:600] kubectl: 1.28.1, cluster: 1.27.4 (minor skew: 1)
I1009 09:06:26.128343   14250 out.go:177] üèÑ  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default

* 
* ==> Docker <==
* Oct 09 06:23:12 minikube cri-dockerd[934]: time="2023-10-09T06:23:12Z" level=info msg="Pulling image mongo-express:latest: e8dad1cef999: Downloading [===============================>                   ]  30.24MB/47.91MB"
Oct 09 06:23:22 minikube cri-dockerd[934]: time="2023-10-09T06:23:22Z" level=info msg="Pulling image mongo-express:latest: e8dad1cef999: Downloading [=====================================>             ]  36.11MB/47.91MB"
Oct 09 06:23:32 minikube cri-dockerd[934]: time="2023-10-09T06:23:32Z" level=info msg="Pulling image mongo-express:latest: e8dad1cef999: Downloading [==========================================>        ]  40.98MB/47.91MB"
Oct 09 06:23:42 minikube cri-dockerd[934]: time="2023-10-09T06:23:42Z" level=info msg="Pulling image mongo-express:latest: e8dad1cef999: Downloading [================================================>  ]  46.31MB/47.91MB"
Oct 09 06:23:52 minikube cri-dockerd[934]: time="2023-10-09T06:23:52Z" level=info msg="Pulling image mongo-express:latest: 7dce78b77cae: Extracting [>                                                  ]  262.1kB/23.44MB"
Oct 09 06:23:59 minikube cri-dockerd[934]: time="2023-10-09T06:23:59Z" level=info msg="Stop pulling image mongo-express:latest: Status: Downloaded newer image for mongo-express:latest"
Oct 09 06:24:04 minikube cri-dockerd[934]: time="2023-10-09T06:24:04Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/kube-webhook-certgen:v20230407@sha256:543c40fd093964bc9ab509d3e791f9989963021f1e9e4c9c7b6700b02bfb227b: Status: Image is up to date for registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:543c40fd093964bc9ab509d3e791f9989963021f1e9e4c9c7b6700b02bfb227b"
Oct 09 06:24:16 minikube dockerd[657]: time="2023-10-09T06:24:16.489179577Z" level=info msg="ignoring event" container=6b7b063b4acd5bc78872d2e8033c9fea1550aeb5f170f8118b760ae26134370b module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 09 06:24:19 minikube dockerd[657]: time="2023-10-09T06:24:19.349999511Z" level=info msg="ignoring event" container=887cbf22619bf8395011c49490699758ee6cb9509f8d8f655e0fc6ef72b9172d module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"
Oct 09 06:26:04 minikube cri-dockerd[934]: time="2023-10-09T06:26:04Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5f549f8991a1002049bbf3b08847294ef624e9b08b99acedc9ac83dccbf46b8b/resolv.conf as [nameserver 10.96.0.10 search ingress-nginx.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 09 06:26:05 minikube dockerd[657]: time="2023-10-09T06:26:05.501157193Z" level=warning msg="reference for unknown type: " digest="sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd" remote="registry.k8s.io/ingress-nginx/controller@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd"
Oct 09 06:26:22 minikube cri-dockerd[934]: time="2023-10-09T06:26:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 8a49fdb3b6a5: Downloading [===========>                                       ]  766.4kB/3.397MB"
Oct 09 06:26:32 minikube cri-dockerd[934]: time="2023-10-09T06:26:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 8a49fdb3b6a5: Downloading [========================>                          ]  1.637MB/3.397MB"
Oct 09 06:26:42 minikube cri-dockerd[934]: time="2023-10-09T06:26:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 8a49fdb3b6a5: Downloading [=======================================>           ]  2.716MB/3.397MB"
Oct 09 06:26:52 minikube cri-dockerd[934]: time="2023-10-09T06:26:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [===>                                               ]  728.5kB/10.02MB"
Oct 09 06:27:02 minikube cri-dockerd[934]: time="2023-10-09T06:27:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [=============>                                     ]  2.609MB/10.02MB"
Oct 09 06:27:12 minikube cri-dockerd[934]: time="2023-10-09T06:27:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [=====================>                             ]   4.28MB/10.02MB"
Oct 09 06:27:22 minikube cri-dockerd[934]: time="2023-10-09T06:27:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [============================>                      ]  5.742MB/10.02MB"
Oct 09 06:27:32 minikube cri-dockerd[934]: time="2023-10-09T06:27:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [==============================>                    ]  6.055MB/10.02MB"
Oct 09 06:27:42 minikube cri-dockerd[934]: time="2023-10-09T06:27:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 91f8bbc316f7: Downloading [===================>                               ]  8.362MB/21.52MB"
Oct 09 06:27:52 minikube cri-dockerd[934]: time="2023-10-09T06:27:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [===============================>                   ]  6.264MB/10.02MB"
Oct 09 06:28:02 minikube cri-dockerd[934]: time="2023-10-09T06:28:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 91f8bbc316f7: Downloading [=======================>                           ]  9.947MB/21.52MB"
Oct 09 06:28:12 minikube cri-dockerd[934]: time="2023-10-09T06:28:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [==================================>                ]  6.891MB/10.02MB"
Oct 09 06:28:22 minikube cri-dockerd[934]: time="2023-10-09T06:28:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 5956894d6526: Downloading [======================================>            ]  7.622MB/10.02MB"
Oct 09 06:28:32 minikube cri-dockerd[934]: time="2023-10-09T06:28:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 91f8bbc316f7: Downloading [=============================>                     ]  12.89MB/21.52MB"
Oct 09 06:28:42 minikube cri-dockerd[934]: time="2023-10-09T06:28:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 91f8bbc316f7: Downloading [==================================>                ]  14.93MB/21.52MB"
Oct 09 06:28:52 minikube cri-dockerd[934]: time="2023-10-09T06:28:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [=================>                                 ]   14.8MB/43.09MB"
Oct 09 06:29:02 minikube cri-dockerd[934]: time="2023-10-09T06:29:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 1c50f5d1f8fa: Downloading [=====================================>             ]  2.156MB/2.842MB"
Oct 09 06:29:12 minikube cri-dockerd[934]: time="2023-10-09T06:29:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [=======>                                           ]   1.95MB/12.89MB"
Oct 09 06:29:22 minikube cri-dockerd[934]: time="2023-10-09T06:29:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [================>                                  ]  4.178MB/12.89MB"
Oct 09 06:29:32 minikube cri-dockerd[934]: time="2023-10-09T06:29:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 91f8bbc316f7: Downloading [=================================================> ]  21.26MB/21.52MB"
Oct 09 06:29:41 minikube cri-dockerd[934]: time="2023-10-09T06:29:41Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/4f88028726b1f61c13ee6b177b48464187b4566bee67dffcddf05d76778d0522/resolv.conf as [nameserver 10.96.0.10 search task-appns.svc.cluster.local svc.cluster.local cluster.local options ndots:5]"
Oct 09 06:29:42 minikube cri-dockerd[934]: time="2023-10-09T06:29:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [=======================>                           ]  20.02MB/43.09MB"
Oct 09 06:29:52 minikube cri-dockerd[934]: time="2023-10-09T06:29:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [===========================>                       ]  7.103MB/12.89MB"
Oct 09 06:30:02 minikube cri-dockerd[934]: time="2023-10-09T06:30:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [================================>                  ]  8.356MB/12.89MB"
Oct 09 06:30:12 minikube cri-dockerd[934]: time="2023-10-09T06:30:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [====================================>              ]   9.47MB/12.89MB"
Oct 09 06:30:22 minikube cri-dockerd[934]: time="2023-10-09T06:30:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Downloading [=======================================>           ]  10.31MB/12.89MB"
Oct 09 06:30:32 minikube cri-dockerd[934]: time="2023-10-09T06:30:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [==========>                                        ]   4.02MB/18.48MB"
Oct 09 06:30:42 minikube cri-dockerd[934]: time="2023-10-09T06:30:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Download complete "
Oct 09 06:30:52 minikube cri-dockerd[934]: time="2023-10-09T06:30:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [==============================>                    ]  26.55MB/43.09MB"
Oct 09 06:31:02 minikube cri-dockerd[934]: time="2023-10-09T06:31:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [===============================>                   ]  27.42MB/43.09MB"
Oct 09 06:31:12 minikube cri-dockerd[934]: time="2023-10-09T06:31:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [==========================>                        ]  9.764MB/18.48MB"
Oct 09 06:31:22 minikube cri-dockerd[934]: time="2023-10-09T06:31:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [=================================>                 ]  29.16MB/43.09MB"
Oct 09 06:31:32 minikube cri-dockerd[934]: time="2023-10-09T06:31:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [================================>                  ]  12.06MB/18.48MB"
Oct 09 06:31:42 minikube cri-dockerd[934]: time="2023-10-09T06:31:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [===================================>               ]  13.02MB/18.48MB"
Oct 09 06:31:52 minikube cri-dockerd[934]: time="2023-10-09T06:31:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [=====================================>             ]  13.79MB/18.48MB"
Oct 09 06:32:02 minikube cri-dockerd[934]: time="2023-10-09T06:32:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [=======================================>           ]  14.74MB/18.48MB"
Oct 09 06:32:12 minikube cri-dockerd[934]: time="2023-10-09T06:32:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [===========================================>       ]  16.08MB/18.48MB"
Oct 09 06:32:22 minikube cri-dockerd[934]: time="2023-10-09T06:32:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 4c9f4e12d18d: Downloading [================================================>  ]     18MB/18.48MB"
Oct 09 06:32:32 minikube cri-dockerd[934]: time="2023-10-09T06:32:32Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [==========================================>        ]  36.99MB/43.09MB"
Oct 09 06:32:42 minikube cri-dockerd[934]: time="2023-10-09T06:32:42Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [============================================>      ]   38.3MB/43.09MB"
Oct 09 06:32:52 minikube cri-dockerd[934]: time="2023-10-09T06:32:52Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [=============================================>     ]  39.17MB/43.09MB"
Oct 09 06:33:02 minikube cri-dockerd[934]: time="2023-10-09T06:33:02Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: f55717a5fb17: Downloading [===============================================>   ]  41.34MB/43.09MB"
Oct 09 06:33:12 minikube cri-dockerd[934]: time="2023-10-09T06:33:12Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: 91f8bbc316f7: Extracting [==============================================>    ]  20.19MB/21.52MB"
Oct 09 06:33:22 minikube cri-dockerd[934]: time="2023-10-09T06:33:22Z" level=info msg="Pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: fcf193613e73: Extracting [==================================================>]  12.89MB/12.89MB"
Oct 09 06:33:29 minikube cri-dockerd[934]: time="2023-10-09T06:33:29Z" level=info msg="Stop pulling image registry.k8s.io/ingress-nginx/controller:v1.8.1@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd: Status: Downloaded newer image for registry.k8s.io/ingress-nginx/controller@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd"
Oct 09 06:33:35 minikube dockerd[657]: time="2023-10-09T06:33:35.646926745Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 09 06:33:35 minikube dockerd[657]: time="2023-10-09T06:33:35.646965725Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"
Oct 09 06:33:57 minikube dockerd[657]: time="2023-10-09T06:33:57.253131267Z" level=error msg="Not continuing with pull after error: errors:\ndenied: requested access to the resource is denied\nunauthorized: authentication required\n"
Oct 09 06:33:57 minikube dockerd[657]: time="2023-10-09T06:33:57.253169824Z" level=info msg="Ignoring extra error returned from registry" error="unauthorized: authentication required"

* 
* ==> container status <==
* CONTAINER           IMAGE                                                                                                                        CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
df39b7db1a265       registry.k8s.io/ingress-nginx/controller@sha256:e5c4824e7375fcf2a393e1c03c293b69759af37a9ca6abdb91b13d78a93da8bd             42 seconds ago      Running             controller                0                   5f549f8991a10       ingress-nginx-controller-7799c6795f-mvzvr
6b7b063b4acd5       registry.k8s.io/ingress-nginx/kube-webhook-certgen@sha256:543c40fd093964bc9ab509d3e791f9989963021f1e9e4c9c7b6700b02bfb227b   10 minutes ago      Exited              create                    0                   887cbf22619bf       ingress-nginx-admission-create-g7p8h
fc62de3f770ca       mongo-express@sha256:b1579aa8b6164281384556b474c61b8ecdbb6cdaa0bd1f7f56964c6117dfce49                                        10 minutes ago      Running             mongo-express             2                   208a79e99b01a       mongo-express-58b7cb7879-bqzjm
4269d0370525a       mongo@sha256:26549961b7dd23ea104ad9ba0c30abec6689fda873070de9fdd2cad304af671d                                                13 minutes ago      Running             mongodb                   0                   27ee6f3666f26       mongodb-deployment-84d7c8b6dd-fqplq
7230968ebdf53       6e38f40d628db                                                                                                                25 minutes ago      Running             storage-provisioner       15                  d293b613cf6fc       storage-provisioner
043f8ab0e0dde       ead0a4a53df89                                                                                                                28 minutes ago      Running             coredns                   5                   cb340b3f89032       coredns-5d78c9869d-rcsqq
9f7c9e5a19c9b       6848d7eda0341                                                                                                                28 minutes ago      Running             kube-proxy                6                   a520d54594b7c       kube-proxy-cgc7x
1b698f878969e       6e38f40d628db                                                                                                                28 minutes ago      Exited              storage-provisioner       14                  d293b613cf6fc       storage-provisioner
2145ab5d4b478       e7972205b6614                                                                                                                30 minutes ago      Running             kube-apiserver            7                   810aa8b1f76c1       kube-apiserver-minikube
df676eef581cf       86b6af7dd652c                                                                                                                30 minutes ago      Running             etcd                      6                   dbe78e302aba7       etcd-minikube
ddeb5f0b70218       98ef2570f3cde                                                                                                                30 minutes ago      Running             kube-scheduler            6                   2e32bdab673c3       kube-scheduler-minikube
77d78ae6608af       f466468864b7a                                                                                                                30 minutes ago      Running             kube-controller-manager   9                   174e0c7175f49       kube-controller-manager-minikube
548686c9605ed       mongo-express@sha256:dcfcf89bf91238ff129469a5a94523b3025913dcc41597d72d4d5f4a0339cc7d                                        4 weeks ago         Exited              mongo-express             1                   51a86a0531877       mongo-express-58b7cb7879-bqzjm
f9d813024c1ba       ead0a4a53df89                                                                                                                4 weeks ago         Exited              coredns                   4                   dfd60e69efbdf       coredns-5d78c9869d-rcsqq
43db99ca2815f       6848d7eda0341                                                                                                                4 weeks ago         Exited              kube-proxy                5                   f0fbbea2f3feb       kube-proxy-cgc7x
a099c60c8a870       e7972205b6614                                                                                                                4 weeks ago         Exited              kube-apiserver            6                   b22e3e13c9299       kube-apiserver-minikube
7c548f5602c2a       86b6af7dd652c                                                                                                                4 weeks ago         Exited              etcd                      5                   33ef5d1581d01       etcd-minikube
2a7f2491e1e4a       f466468864b7a                                                                                                                4 weeks ago         Exited              kube-controller-manager   8                   016a556e20577       kube-controller-manager-minikube
eac185715e7f2       98ef2570f3cde                                                                                                                4 weeks ago         Exited              kube-scheduler            5                   89955883f3b2f       kube-scheduler-minikube

* 
* ==> coredns [043f8ab0e0dd] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 75e5db48a73272e2c90919c8256e5cca0293ae0ed689e2ed44f1254a9589c3d004cb3e693d059116718c47e9305987b828b11b2735a1cefa59e4a9489dda5cee
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:43694 - 3021 "HINFO IN 612248604060299882.4023729830883199488. udp 56 false 512" NXDOMAIN qr,rd,ra 56 2.095141234s
[INFO] 127.0.0.1:35168 - 9550 "HINFO IN 612248604060299882.4023729830883199488. udp 56 false 512" NXDOMAIN qr,rd,ra 56 5.044875465s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:33520 - 21301 "HINFO IN 612248604060299882.4023729830883199488. udp 56 false 512" NXDOMAIN qr,rd,ra 56 0.004645187s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 10.244.0.23:53194 - 53559 "A IN compass.mongodb.com.default.svc.cluster.local. udp 63 false 512" NXDOMAIN qr,aa,rd 156 0.000232067s
[INFO] 10.244.0.23:47471 - 60444 "A IN compass.mongodb.com.svc.cluster.local. udp 55 false 512" NXDOMAIN qr,aa,rd 148 0.000118901s
[INFO] 10.244.0.23:43293 - 6519 "A IN compass.mongodb.com.cluster.local. udp 51 false 512" NXDOMAIN qr,aa,rd 144 0.00011099s
[INFO] 10.244.0.23:38756 - 61047 "A IN compass.mongodb.com. udp 37 false 512" NOERROR qr,rd,ra 743 0.08125411s
[INFO] 10.244.0.23:37565 - 49896 "A IN api.segment.io.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000180171s
[INFO] 10.244.0.23:38337 - 35870 "A IN api.segment.io.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000138606s
[INFO] 10.244.0.23:45480 - 31850 "A IN api.segment.io.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.000133025s
[INFO] 10.244.0.23:59385 - 5505 "A IN api.segment.io. udp 32 false 512" NOERROR qr,rd,ra 559 0.022427074s
[INFO] 10.244.0.23:50390 - 35807 "A IN api.segment.io.default.svc.cluster.local. udp 58 false 512" NXDOMAIN qr,aa,rd 151 0.000105467s
[INFO] 10.244.0.23:55654 - 61074 "A IN api.segment.io.svc.cluster.local. udp 50 false 512" NXDOMAIN qr,aa,rd 143 0.000055387s
[INFO] 10.244.0.23:55227 - 4000 "A IN api.segment.io.cluster.local. udp 46 false 512" NXDOMAIN qr,aa,rd 139 0.00005599s
[INFO] 10.244.0.23:42259 - 45153 "A IN api.segment.io. udp 32 false 512" NOERROR qr,aa,rd,ra 559 0.000084367s
[INFO] 10.244.0.24:40570 - 9184 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000241816s
[INFO] 10.244.0.24:53448 - 31525 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.00008313s
[INFO] 10.244.0.24:34340 - 24572 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000080189s

* 
* ==> coredns [f9d813024c1b] <==
* [INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 75e5db48a73272e2c90919c8256e5cca0293ae0ed689e2ed44f1254a9589c3d004cb3e693d059116718c47e9305987b828b11b2735a1cefa59e4a9489dda5cee
CoreDNS-1.10.1
linux/amd64, go1.20, 055b2c3
[INFO] 127.0.0.1:37685 - 47914 "HINFO IN 6999609481701507866.5775468433847856661. udp 57 false 512" NXDOMAIN qr,rd,ra 132 1.8802472099999998s
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[WARNING] plugin/kubernetes: Kubernetes API connection failure: Get "https://10.96.0.1:443/version": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 10.244.0.19:60979 - 27479 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000391151s
[INFO] 10.244.0.19:54344 - 45380 "A IN mongodb-service.default.svc.cluster.local. udp 59 false 512" NOERROR qr,aa,rd 116 0.000190982s
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s

* 
* ==> describe nodes <==
* Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=fd7ecd9c4599bef9f04c0986c4a0187f98a4396e
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2023_09_02T18_09_29_0700
                    minikube.k8s.io/version=v1.31.2
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 02 Sep 2023 15:09:12 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 09 Oct 2023 06:34:07 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 09 Oct 2023 06:33:56 +0000   Sun, 03 Sep 2023 08:45:56 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 09 Oct 2023 06:33:56 +0000   Sun, 03 Sep 2023 08:45:56 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 09 Oct 2023 06:33:56 +0000   Sun, 03 Sep 2023 08:45:56 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 09 Oct 2023 06:33:56 +0000   Sun, 03 Sep 2023 08:45:56 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.58.2
  Hostname:    minikube
Capacity:
  cpu:                4
  ephemeral-storage:  479079112Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12117608Ki
  pods:               110
Allocatable:
  cpu:                4
  ephemeral-storage:  479079112Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             12117608Ki
  pods:               110
System Info:
  Machine ID:                 0bb7db086564472f8c8f9b91b1d01159
  System UUID:                048c28a3-bef7-4edc-8880-47a8d5759242
  Boot ID:                    50649f49-0638-4849-9f76-802303227b12
  Kernel Version:             6.2.0-34-generic
  OS Image:                   Ubuntu 22.04.2 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://24.0.4
  Kubelet Version:            v1.27.4
  Kube-Proxy Version:         v1.27.4
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (11 in total)
  Namespace                   Name                                         CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                         ------------  ----------  ---------------  -------------  ---
  default                     mongo-express-58b7cb7879-bqzjm               0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34d
  default                     mongodb-deployment-84d7c8b6dd-fqplq          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         34d
  ingress-nginx               ingress-nginx-controller-7799c6795f-mvzvr    100m (2%!)(MISSING)     0 (0%!)(MISSING)      90Mi (0%!)(MISSING)        0 (0%!)(MISSING)         34d
  kube-system                 coredns-5d78c9869d-rcsqq                     100m (2%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     36d
  kube-system                 etcd-minikube                                100m (2%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         36d
  kube-system                 kube-apiserver-minikube                      250m (6%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         36d
  kube-system                 kube-controller-manager-minikube             200m (5%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         36d
  kube-system                 kube-proxy-cgc7x                             0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         36d
  kube-system                 kube-scheduler-minikube                      100m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         36d
  kube-system                 storage-provisioner                          0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         36d
  task-appns                  task-app-deployment-5f68b76c59-wj49n         0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         4m37s
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                850m (21%!)(MISSING)  0 (0%!)(MISSING)
  memory             260Mi (2%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 27m                kube-proxy       
  Normal  Starting                 30m                kubelet          Starting kubelet.
  Normal  NodeHasSufficientPID     30m (x7 over 30m)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  30m                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  30m (x8 over 30m)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    30m (x8 over 30m)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  RegisteredNode           28m                node-controller  Node minikube event: Registered Node minikube in Controller

* 
* ==> dmesg <==
* [  +0.014434] I/O error, dev sda, sector 419451440 op 0x0:(READ) flags 0x0 phys_seg 1 prio class 2
[  +3.932757] kauditd_printk_skb: 43 callbacks suppressed
[  +2.087367] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000009] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000004] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000003] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000003] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000001] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000002] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[ +11.477855] vboxdrv: loading out-of-tree module taints kernel.
[  +0.060978] VBoxNetFlt: Successfully started.
[  +0.293927] VBoxNetAdp: Successfully started.
[Oct 9 04:20] atkbd serio0: Unknown key pressed (translated set 2, code 0xa7 on isa0060/serio0).
[  +0.000007] atkbd serio0: Use 'setkeycodes e027 <keycode>' to make it known.
[Oct 9 04:42] atkbd serio0: Unknown key pressed (translated set 2, code 0xf8 on isa0060/serio0).
[  +0.000005] atkbd serio0: Use 'setkeycodes e078 <keycode>' to make it known.
[  +0.477416] atkbd serio0: Unknown key released (translated set 2, code 0xf8 on isa0060/serio0).
[  +0.000008] atkbd serio0: Use 'setkeycodes e078 <keycode>' to make it known.
[Oct 9 05:00] acpi PNP0501:00: Still not present
[  +0.016785] done.
[  +3.788517] ata5.00: exception Emask 0x0 SAct 0x380 SErr 0x40000 action 0x0
[  +0.000008] ata5.00: irq_stat 0x40000008
[  +0.000002] ata5: SError: { CommWake }
[  +0.000003] ata5.00: failed command: READ FPDMA QUEUED
[  +0.000002] ata5.00: cmd 60/08:38:30:52:00/00:00:19:00:00/40 tag 7 ncq dma 4096 in
                       res 41/40:08:30:52:00/00:00:19:00:00/40 Emask 0x409 (media error) <F>
[  +0.000006] ata5.00: status: { DRDY ERR }
[  +0.000002] ata5.00: error: { UNC }
[  +0.013963] I/O error, dev sda, sector 419451440 op 0x0:(READ) flags 0x0 phys_seg 1 prio class 2
[  +3.457776] ata5.00: exception Emask 0x0 SAct 0x1c000 SErr 0x40000 action 0x0
[  +0.000009] ata5.00: irq_stat 0x40000008
[  +0.000002] ata5: SError: { CommWake }
[  +0.000003] ata5.00: failed command: READ FPDMA QUEUED
[  +0.000002] ata5.00: cmd 60/08:80:30:52:00/00:00:19:00:00/40 tag 16 ncq dma 4096 in
                       res 41/40:08:30:52:00/00:00:19:00:00/40 Emask 0x409 (media error) <F>
[  +0.000006] ata5.00: status: { DRDY ERR }
[  +0.000002] ata5.00: error: { UNC }
[  +0.013923] I/O error, dev sda, sector 419451440 op 0x0:(READ) flags 0x0 phys_seg 1 prio class 2
[  +3.541878] ata5.00: exception Emask 0x0 SAct 0x600081e SErr 0x40000 action 0x0
[  +0.000008] ata5.00: irq_stat 0x40000008
[  +0.000002] ata5: SError: { CommWake }
[  +0.000003] ata5.00: failed command: READ FPDMA QUEUED
[  +0.000002] ata5.00: cmd 60/08:10:30:52:00/00:00:19:00:00/40 tag 2 ncq dma 4096 in
                       res 41/40:08:30:52:00/00:00:19:00:00/40 Emask 0x409 (media error) <F>
[  +0.000005] ata5.00: status: { DRDY ERR }
[  +0.000002] ata5.00: error: { UNC }
[  +0.013972] I/O error, dev sda, sector 419451440 op 0x0:(READ) flags 0x0 phys_seg 1 prio class 2
[  +0.777091] atkbd serio0: Unknown key pressed (translated set 2, code 0xf8 on isa0060/serio0).
[  +0.000006] atkbd serio0: Use 'setkeycodes e078 <keycode>' to make it known.
[  +0.150683] atkbd serio0: Unknown key released (translated set 2, code 0xf8 on isa0060/serio0).
[  +0.000009] atkbd serio0: Use 'setkeycodes e078 <keycode>' to make it known.
[  +0.089207] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000024] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000003] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000003] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000002] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000002] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[  +0.000003] ACPI: \: failed to evaluate _DSM bf0212f2-788f-c64d-a5b3-1f738e285ade (0x1001)
[Oct 9 05:44] atkbd serio0: Unknown key pressed (translated set 2, code 0xa5 on isa0060/serio0).
[  +0.000014] atkbd serio0: Use 'setkeycodes e025 <keycode>' to make it known.

* 
* ==> etcd [7c548f5602c2] <==
* {"level":"info","ts":"2023-09-04T20:03:47.933Z","caller":"traceutil/trace.go:171","msg":"trace[2104767403] transaction","detail":"{read_only:false; response_revision:43770; number_of_response:1; }","duration":"323.317716ms","start":"2023-09-04T20:03:47.610Z","end":"2023-09-04T20:03:47.933Z","steps":["trace[2104767403] 'process raft request'  (duration: 184.026779ms)","trace[2104767403] 'compare'  (duration: 139.078613ms)"],"step_count":2}
{"level":"warn","ts":"2023-09-04T20:03:47.933Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-09-04T20:03:47.610Z","time spent":"323.416667ms","remote":"127.0.0.1:36046","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":521,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-node-lease/minikube\" mod_revision:43762 > success:<request_put:<key:\"/registry/leases/kube-node-lease/minikube\" value_size:472 >> failure:<request_range:<key:\"/registry/leases/kube-node-lease/minikube\" > >"}
{"level":"info","ts":"2023-09-04T20:03:49.869Z","caller":"traceutil/trace.go:171","msg":"trace[163290465] transaction","detail":"{read_only:false; response_revision:43771; number_of_response:1; }","duration":"111.079227ms","start":"2023-09-04T20:03:49.758Z","end":"2023-09-04T20:03:49.869Z","steps":["trace[163290465] 'process raft request'  (duration: 110.971ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:03:52.065Z","caller":"traceutil/trace.go:171","msg":"trace[1832678372] transaction","detail":"{read_only:false; response_revision:43772; number_of_response:1; }","duration":"189.101406ms","start":"2023-09-04T20:03:51.876Z","end":"2023-09-04T20:03:52.065Z","steps":["trace[1832678372] 'process raft request'  (duration: 188.978974ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:03:54.192Z","caller":"traceutil/trace.go:171","msg":"trace[769529004] linearizableReadLoop","detail":"{readStateIndex:55170; appliedIndex:55169; }","duration":"105.995902ms","start":"2023-09-04T20:03:54.086Z","end":"2023-09-04T20:03:54.192Z","steps":["trace[769529004] 'read index received'  (duration: 105.896537ms)","trace[769529004] 'applied index is now lower than readState.Index'  (duration: 98.789¬µs)"],"step_count":2}
{"level":"info","ts":"2023-09-04T20:03:54.192Z","caller":"traceutil/trace.go:171","msg":"trace[607528836] transaction","detail":"{read_only:false; response_revision:43773; number_of_response:1; }","duration":"120.111163ms","start":"2023-09-04T20:03:54.072Z","end":"2023-09-04T20:03:54.192Z","steps":["trace[607528836] 'process raft request'  (duration: 119.935566ms)"],"step_count":1}
{"level":"warn","ts":"2023-09-04T20:03:54.192Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"106.113536ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-09-04T20:03:54.192Z","caller":"traceutil/trace.go:171","msg":"trace[1025284128] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:43773; }","duration":"106.159482ms","start":"2023-09-04T20:03:54.086Z","end":"2023-09-04T20:03:54.192Z","steps":["trace[1025284128] 'agreement among raft nodes before linearized reading'  (duration: 106.054859ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:03:56.218Z","caller":"traceutil/trace.go:171","msg":"trace[1516236030] transaction","detail":"{read_only:false; response_revision:43775; number_of_response:1; }","duration":"108.522583ms","start":"2023-09-04T20:03:56.110Z","end":"2023-09-04T20:03:56.218Z","steps":["trace[1516236030] 'process raft request'  (duration: 108.360213ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:02.515Z","caller":"traceutil/trace.go:171","msg":"trace[1642227057] transaction","detail":"{read_only:false; response_revision:43780; number_of_response:1; }","duration":"189.105817ms","start":"2023-09-04T20:04:02.326Z","end":"2023-09-04T20:04:02.515Z","steps":["trace[1642227057] 'process raft request'  (duration: 189.00836ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:06.429Z","caller":"traceutil/trace.go:171","msg":"trace[856978680] transaction","detail":"{read_only:false; response_revision:43783; number_of_response:1; }","duration":"130.537777ms","start":"2023-09-04T20:04:06.298Z","end":"2023-09-04T20:04:06.429Z","steps":["trace[856978680] 'process raft request'  (duration: 130.444491ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:08.596Z","caller":"traceutil/trace.go:171","msg":"trace[347058709] transaction","detail":"{read_only:false; response_revision:43785; number_of_response:1; }","duration":"127.671525ms","start":"2023-09-04T20:04:08.469Z","end":"2023-09-04T20:04:08.596Z","steps":["trace[347058709] 'process raft request'  (duration: 127.51047ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:10.885Z","caller":"traceutil/trace.go:171","msg":"trace[1736939237] transaction","detail":"{read_only:false; response_revision:43787; number_of_response:1; }","duration":"178.47365ms","start":"2023-09-04T20:04:10.706Z","end":"2023-09-04T20:04:10.884Z","steps":["trace[1736939237] 'process raft request'  (duration: 178.320778ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:15.064Z","caller":"traceutil/trace.go:171","msg":"trace[2128263166] transaction","detail":"{read_only:false; response_revision:43790; number_of_response:1; }","duration":"116.332678ms","start":"2023-09-04T20:04:14.948Z","end":"2023-09-04T20:04:15.064Z","steps":["trace[2128263166] 'process raft request'  (duration: 116.24234ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:16.953Z","caller":"traceutil/trace.go:171","msg":"trace[382502058] transaction","detail":"{read_only:false; response_revision:43791; number_of_response:1; }","duration":"158.36418ms","start":"2023-09-04T20:04:16.795Z","end":"2023-09-04T20:04:16.953Z","steps":["trace[382502058] 'process raft request'  (duration: 158.244649ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:19.110Z","caller":"traceutil/trace.go:171","msg":"trace[55902128] transaction","detail":"{read_only:false; response_revision:43793; number_of_response:1; }","duration":"122.481119ms","start":"2023-09-04T20:04:18.988Z","end":"2023-09-04T20:04:19.110Z","steps":["trace[55902128] 'process raft request'  (duration: 121.947364ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:19.264Z","caller":"traceutil/trace.go:171","msg":"trace[834061147] transaction","detail":"{read_only:false; response_revision:43794; number_of_response:1; }","duration":"112.321478ms","start":"2023-09-04T20:04:19.152Z","end":"2023-09-04T20:04:19.264Z","steps":["trace[834061147] 'process raft request'  (duration: 90.122474ms)","trace[834061147] 'compare'  (duration: 22.001798ms)"],"step_count":2}
{"level":"info","ts":"2023-09-04T20:04:23.513Z","caller":"traceutil/trace.go:171","msg":"trace[791292752] transaction","detail":"{read_only:false; response_revision:43797; number_of_response:1; }","duration":"170.495324ms","start":"2023-09-04T20:04:23.343Z","end":"2023-09-04T20:04:23.513Z","steps":["trace[791292752] 'process raft request'  (duration: 170.39609ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:24.532Z","caller":"traceutil/trace.go:171","msg":"trace[1325254518] transaction","detail":"{read_only:false; response_revision:43799; number_of_response:1; }","duration":"161.04323ms","start":"2023-09-04T20:04:24.371Z","end":"2023-09-04T20:04:24.532Z","steps":["trace[1325254518] 'process raft request'  (duration: 117.685923ms)","trace[1325254518] 'compare'  (duration: 42.954481ms)"],"step_count":2}
{"level":"info","ts":"2023-09-04T20:04:25.623Z","caller":"traceutil/trace.go:171","msg":"trace[1359307605] transaction","detail":"{read_only:false; response_revision:43800; number_of_response:1; }","duration":"101.554636ms","start":"2023-09-04T20:04:25.521Z","end":"2023-09-04T20:04:25.623Z","steps":["trace[1359307605] 'process raft request'  (duration: 101.399406ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:27.738Z","caller":"traceutil/trace.go:171","msg":"trace[1763461045] transaction","detail":"{read_only:false; response_revision:43802; number_of_response:1; }","duration":"108.392039ms","start":"2023-09-04T20:04:27.630Z","end":"2023-09-04T20:04:27.738Z","steps":["trace[1763461045] 'process raft request'  (duration: 108.248235ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:29.368Z","caller":"traceutil/trace.go:171","msg":"trace[871099084] transaction","detail":"{read_only:false; response_revision:43803; number_of_response:1; }","duration":"115.955284ms","start":"2023-09-04T20:04:29.252Z","end":"2023-09-04T20:04:29.368Z","steps":["trace[871099084] 'process raft request'  (duration: 115.772157ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:29.857Z","caller":"traceutil/trace.go:171","msg":"trace[1840904587] transaction","detail":"{read_only:false; response_revision:43804; number_of_response:1; }","duration":"112.537828ms","start":"2023-09-04T20:04:29.745Z","end":"2023-09-04T20:04:29.857Z","steps":["trace[1840904587] 'process raft request'  (duration: 112.42969ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:32.042Z","caller":"traceutil/trace.go:171","msg":"trace[1439509901] transaction","detail":"{read_only:false; response_revision:43806; number_of_response:1; }","duration":"178.052806ms","start":"2023-09-04T20:04:31.864Z","end":"2023-09-04T20:04:32.042Z","steps":["trace[1439509901] 'process raft request'  (duration: 177.942251ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:33.467Z","caller":"mvcc/index.go:214","msg":"compact tree index","revision":43561}
{"level":"info","ts":"2023-09-04T20:04:33.468Z","caller":"mvcc/kvstore_compaction.go:66","msg":"finished scheduled compaction","compact-revision":43561,"took":"805.858¬µs","hash":1975152697}
{"level":"info","ts":"2023-09-04T20:04:33.468Z","caller":"mvcc/hash.go:137","msg":"storing new hash","hash":1975152697,"revision":43561,"compact-revision":43317}
{"level":"info","ts":"2023-09-04T20:04:34.158Z","caller":"traceutil/trace.go:171","msg":"trace[2022048972] transaction","detail":"{read_only:false; response_revision:43808; number_of_response:1; }","duration":"111.111302ms","start":"2023-09-04T20:04:34.047Z","end":"2023-09-04T20:04:34.158Z","steps":["trace[2022048972] 'process raft request'  (duration: 110.866395ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:36.281Z","caller":"traceutil/trace.go:171","msg":"trace[1387825170] transaction","detail":"{read_only:false; response_revision:43810; number_of_response:1; }","duration":"116.601792ms","start":"2023-09-04T20:04:36.164Z","end":"2023-09-04T20:04:36.281Z","steps":["trace[1387825170] 'process raft request'  (duration: 116.489425ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:40.438Z","caller":"traceutil/trace.go:171","msg":"trace[19650154] transaction","detail":"{read_only:false; response_revision:43814; number_of_response:1; }","duration":"125.621311ms","start":"2023-09-04T20:04:40.312Z","end":"2023-09-04T20:04:40.438Z","steps":["trace[19650154] 'process raft request'  (duration: 125.345256ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:42.613Z","caller":"traceutil/trace.go:171","msg":"trace[1520136044] transaction","detail":"{read_only:false; response_revision:43815; number_of_response:1; }","duration":"167.502677ms","start":"2023-09-04T20:04:42.445Z","end":"2023-09-04T20:04:42.613Z","steps":["trace[1520136044] 'process raft request'  (duration: 167.418328ms)"],"step_count":1}
{"level":"warn","ts":"2023-09-04T20:04:44.559Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"194.680848ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238521749843173644 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/masterleases/192.168.58.2\" mod_revision:43809 > success:<request_put:<key:\"/registry/masterleases/192.168.58.2\" value_size:66 lease:3238521749843173642 >> failure:<request_range:<key:\"/registry/masterleases/192.168.58.2\" > >>","response":"size:18"}
{"level":"info","ts":"2023-09-04T20:04:44.559Z","caller":"traceutil/trace.go:171","msg":"trace[991028822] transaction","detail":"{read_only:false; response_revision:43816; number_of_response:1; }","duration":"205.21806ms","start":"2023-09-04T20:04:44.354Z","end":"2023-09-04T20:04:44.559Z","steps":["trace[991028822] 'process raft request'  (duration: 10.418821ms)","trace[991028822] 'compare'  (duration: 194.592604ms)"],"step_count":2}
{"level":"info","ts":"2023-09-04T20:04:45.167Z","caller":"traceutil/trace.go:171","msg":"trace[1008183621] transaction","detail":"{read_only:false; response_revision:43818; number_of_response:1; }","duration":"173.411534ms","start":"2023-09-04T20:04:44.993Z","end":"2023-09-04T20:04:45.167Z","steps":["trace[1008183621] 'process raft request'  (duration: 173.251012ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:46.795Z","caller":"traceutil/trace.go:171","msg":"trace[1639985412] transaction","detail":"{read_only:false; response_revision:43819; number_of_response:1; }","duration":"146.834068ms","start":"2023-09-04T20:04:46.648Z","end":"2023-09-04T20:04:46.795Z","steps":["trace[1639985412] 'process raft request'  (duration: 146.728932ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:47.827Z","caller":"traceutil/trace.go:171","msg":"trace[990294979] transaction","detail":"{read_only:false; response_revision:43820; number_of_response:1; }","duration":"191.324903ms","start":"2023-09-04T20:04:47.635Z","end":"2023-09-04T20:04:47.827Z","steps":["trace[990294979] 'process raft request'  (duration: 191.238161ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:48.940Z","caller":"traceutil/trace.go:171","msg":"trace[99955501] transaction","detail":"{read_only:false; response_revision:43821; number_of_response:1; }","duration":"138.967923ms","start":"2023-09-04T20:04:48.801Z","end":"2023-09-04T20:04:48.940Z","steps":["trace[99955501] 'process raft request'  (duration: 138.881041ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:50.094Z","caller":"traceutil/trace.go:171","msg":"trace[1623848573] transaction","detail":"{read_only:false; response_revision:43822; number_of_response:1; }","duration":"102.26714ms","start":"2023-09-04T20:04:49.992Z","end":"2023-09-04T20:04:50.094Z","steps":["trace[1623848573] 'process raft request'  (duration: 102.121616ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:51.074Z","caller":"traceutil/trace.go:171","msg":"trace[4223641] transaction","detail":"{read_only:false; response_revision:43824; number_of_response:1; }","duration":"128.194322ms","start":"2023-09-04T20:04:50.946Z","end":"2023-09-04T20:04:51.074Z","steps":["trace[4223641] 'process raft request'  (duration: 127.838549ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:04:53.284Z","caller":"traceutil/trace.go:171","msg":"trace[335169181] transaction","detail":"{read_only:false; response_revision:43825; number_of_response:1; }","duration":"204.982662ms","start":"2023-09-04T20:04:53.079Z","end":"2023-09-04T20:04:53.284Z","steps":["trace[335169181] 'process raft request'  (duration: 204.875444ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:00.554Z","caller":"traceutil/trace.go:171","msg":"trace[1557385620] transaction","detail":"{read_only:false; response_revision:43831; number_of_response:1; }","duration":"115.041276ms","start":"2023-09-04T20:05:00.439Z","end":"2023-09-04T20:05:00.554Z","steps":["trace[1557385620] 'process raft request'  (duration: 114.704284ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:01.626Z","caller":"traceutil/trace.go:171","msg":"trace[1695137413] transaction","detail":"{read_only:false; response_revision:43832; number_of_response:1; }","duration":"166.732817ms","start":"2023-09-04T20:05:01.459Z","end":"2023-09-04T20:05:01.626Z","steps":["trace[1695137413] 'process raft request'  (duration: 166.611691ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:03.755Z","caller":"traceutil/trace.go:171","msg":"trace[142744094] transaction","detail":"{read_only:false; response_revision:43833; number_of_response:1; }","duration":"116.476382ms","start":"2023-09-04T20:05:03.639Z","end":"2023-09-04T20:05:03.755Z","steps":["trace[142744094] 'process raft request'  (duration: 116.17683ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:05.111Z","caller":"traceutil/trace.go:171","msg":"trace[1404917046] transaction","detail":"{read_only:false; response_revision:43835; number_of_response:1; }","duration":"115.957711ms","start":"2023-09-04T20:05:04.995Z","end":"2023-09-04T20:05:05.111Z","steps":["trace[1404917046] 'process raft request'  (duration: 115.835359ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:05.253Z","caller":"traceutil/trace.go:171","msg":"trace[1317028036] transaction","detail":"{read_only:false; response_revision:43836; number_of_response:1; }","duration":"255.390889ms","start":"2023-09-04T20:05:04.998Z","end":"2023-09-04T20:05:05.253Z","steps":["trace[1317028036] 'process raft request'  (duration: 207.443088ms)","trace[1317028036] 'compare'  (duration: 47.861422ms)"],"step_count":2}
{"level":"info","ts":"2023-09-04T20:05:05.928Z","caller":"traceutil/trace.go:171","msg":"trace[1865012891] transaction","detail":"{read_only:false; response_revision:43837; number_of_response:1; }","duration":"162.164578ms","start":"2023-09-04T20:05:05.766Z","end":"2023-09-04T20:05:05.928Z","steps":["trace[1865012891] 'process raft request'  (duration: 162.064071ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:08.056Z","caller":"traceutil/trace.go:171","msg":"trace[1728594160] transaction","detail":"{read_only:false; response_revision:43838; number_of_response:1; }","duration":"116.840194ms","start":"2023-09-04T20:05:07.939Z","end":"2023-09-04T20:05:08.056Z","steps":["trace[1728594160] 'process raft request'  (duration: 116.71227ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:08.567Z","caller":"traceutil/trace.go:171","msg":"trace[373814005] transaction","detail":"{read_only:false; response_revision:43839; number_of_response:1; }","duration":"121.905712ms","start":"2023-09-04T20:05:08.446Z","end":"2023-09-04T20:05:08.567Z","steps":["trace[373814005] 'process raft request'  (duration: 121.676674ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:10.267Z","caller":"traceutil/trace.go:171","msg":"trace[830829942] linearizableReadLoop","detail":"{readStateIndex:55255; appliedIndex:55254; }","duration":"169.691848ms","start":"2023-09-04T20:05:10.097Z","end":"2023-09-04T20:05:10.267Z","steps":["trace[830829942] 'read index received'  (duration: 169.366531ms)","trace[830829942] 'applied index is now lower than readState.Index'  (duration: 322.715¬µs)"],"step_count":2}
{"level":"info","ts":"2023-09-04T20:05:10.267Z","caller":"traceutil/trace.go:171","msg":"trace[1878427962] transaction","detail":"{read_only:false; response_revision:43840; number_of_response:1; }","duration":"197.406209ms","start":"2023-09-04T20:05:10.070Z","end":"2023-09-04T20:05:10.267Z","steps":["trace[1878427962] 'process raft request'  (duration: 196.918233ms)"],"step_count":1}
{"level":"warn","ts":"2023-09-04T20:05:10.267Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"170.094107ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-09-04T20:05:10.267Z","caller":"traceutil/trace.go:171","msg":"trace[904014209] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:43840; }","duration":"170.22705ms","start":"2023-09-04T20:05:10.097Z","end":"2023-09-04T20:05:10.267Z","steps":["trace[904014209] 'agreement among raft nodes before linearized reading'  (duration: 169.897652ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:11.001Z","caller":"traceutil/trace.go:171","msg":"trace[1616934945] transaction","detail":"{read_only:false; response_revision:43841; number_of_response:1; }","duration":"111.873446ms","start":"2023-09-04T20:05:10.889Z","end":"2023-09-04T20:05:11.001Z","steps":["trace[1616934945] 'process raft request'  (duration: 111.76804ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:12.472Z","caller":"traceutil/trace.go:171","msg":"trace[676906305] transaction","detail":"{read_only:false; response_revision:43842; number_of_response:1; }","duration":"196.39191ms","start":"2023-09-04T20:05:12.276Z","end":"2023-09-04T20:05:12.472Z","steps":["trace[676906305] 'process raft request'  (duration: 196.264345ms)"],"step_count":1}
{"level":"info","ts":"2023-09-04T20:05:12.947Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2023-09-04T20:05:12.947Z","caller":"embed/etcd.go:373","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}
{"level":"info","ts":"2023-09-04T20:05:13.010Z","caller":"etcdserver/server.go:1465","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"b2c6679ac05f2cf1","current-leader-member-id":"b2c6679ac05f2cf1"}
{"level":"info","ts":"2023-09-04T20:05:13.151Z","caller":"embed/etcd.go:568","msg":"stopping serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2023-09-04T20:05:13.151Z","caller":"embed/etcd.go:573","msg":"stopped serving peer traffic","address":"192.168.58.2:2380"}
{"level":"info","ts":"2023-09-04T20:05:13.151Z","caller":"embed/etcd.go:375","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.58.2:2380"],"advertise-client-urls":["https://192.168.58.2:2379"]}

* 
* ==> etcd [df676eef581c] <==
* {"level":"warn","ts":"2023-10-09T06:33:24.288Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-09T06:33:23.807Z","time spent":"480.618703ms","remote":"127.0.0.1:57056","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-10-09T06:33:24.288Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-09T06:33:23.732Z","time spent":"555.990283ms","remote":"127.0.0.1:57258","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":673,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" mod_revision:45319 > success:<request_put:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" value_size:600 >> failure:<request_range:<key:\"/registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii\" > >"}
{"level":"warn","ts":"2023-10-09T06:33:25.191Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"401.140185ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238522520939074507 > lease_revoke:<id:2cf18b1309ad138c>","response":"size:29"}
{"level":"info","ts":"2023-10-09T06:33:25.191Z","caller":"traceutil/trace.go:171","msg":"trace[1417518125] linearizableReadLoop","detail":"{readStateIndex:57117; appliedIndex:57116; }","duration":"895.119581ms","start":"2023-10-09T06:33:24.296Z","end":"2023-10-09T06:33:25.191Z","steps":["trace[1417518125] 'read index received'  (duration: 493.882975ms)","trace[1417518125] 'applied index is now lower than readState.Index'  (duration: 401.232865ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-09T06:33:25.191Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"895.250035ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-10-09T06:33:25.191Z","caller":"traceutil/trace.go:171","msg":"trace[778181602] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:45326; }","duration":"895.299779ms","start":"2023-10-09T06:33:24.296Z","end":"2023-10-09T06:33:25.191Z","steps":["trace[778181602] 'agreement among raft nodes before linearized reading'  (duration: 895.190275ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-09T06:33:25.191Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-09T06:33:24.295Z","time spent":"895.349172ms","remote":"127.0.0.1:57054","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-10-09T06:33:25.191Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"437.838829ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/endpointslices/\" range_end:\"/registry/endpointslices0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"warn","ts":"2023-10-09T06:33:25.191Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"385.061122ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" ","response":"range_response_count:1 size:1112"}
{"level":"info","ts":"2023-10-09T06:33:25.191Z","caller":"traceutil/trace.go:171","msg":"trace[550637436] range","detail":"{range_begin:/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath; range_end:; response_count:1; response_revision:45326; }","duration":"385.087995ms","start":"2023-10-09T06:33:24.806Z","end":"2023-10-09T06:33:25.191Z","steps":["trace[550637436] 'agreement among raft nodes before linearized reading'  (duration: 385.013519ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:25.191Z","caller":"traceutil/trace.go:171","msg":"trace[1670496619] range","detail":"{range_begin:/registry/endpointslices/; range_end:/registry/endpointslices0; response_count:0; response_revision:45326; }","duration":"437.940962ms","start":"2023-10-09T06:33:24.753Z","end":"2023-10-09T06:33:25.191Z","steps":["trace[1670496619] 'agreement among raft nodes before linearized reading'  (duration: 437.79747ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-09T06:33:25.191Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-09T06:33:24.806Z","time spent":"385.13048ms","remote":"127.0.0.1:57172","response type":"/etcdserverpb.KV/Range","request count":0,"request size":67,"response count":1,"response size":1135,"request content":"key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" "}
{"level":"warn","ts":"2023-10-09T06:33:25.191Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-09T06:33:24.753Z","time spent":"437.999237ms","remote":"127.0.0.1:57266","response type":"/etcdserverpb.KV/Range","request count":0,"request size":56,"response count":7,"response size":31,"request content":"key:\"/registry/endpointslices/\" range_end:\"/registry/endpointslices0\" count_only:true "}
{"level":"info","ts":"2023-10-09T06:33:25.766Z","caller":"traceutil/trace.go:171","msg":"trace[1237438471] linearizableReadLoop","detail":"{readStateIndex:57118; appliedIndex:57117; }","duration":"274.044987ms","start":"2023-10-09T06:33:25.492Z","end":"2023-10-09T06:33:25.766Z","steps":["trace[1237438471] 'read index received'  (duration: 218.621671ms)","trace[1237438471] 'applied index is now lower than readState.Index'  (duration: 55.422166ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-09T06:33:25.766Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"274.184269ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/clusterrolebindings/\" range_end:\"/registry/clusterrolebindings0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-10-09T06:33:25.766Z","caller":"traceutil/trace.go:171","msg":"trace[204196566] transaction","detail":"{read_only:false; response_revision:45327; number_of_response:1; }","duration":"570.504922ms","start":"2023-10-09T06:33:25.195Z","end":"2023-10-09T06:33:25.766Z","steps":["trace[204196566] 'process raft request'  (duration: 514.885221ms)","trace[204196566] 'compare'  (duration: 55.36883ms)"],"step_count":2}
{"level":"info","ts":"2023-10-09T06:33:25.766Z","caller":"traceutil/trace.go:171","msg":"trace[495191077] range","detail":"{range_begin:/registry/clusterrolebindings/; range_end:/registry/clusterrolebindings0; response_count:0; response_revision:45327; }","duration":"274.234429ms","start":"2023-10-09T06:33:25.492Z","end":"2023-10-09T06:33:25.766Z","steps":["trace[495191077] 'agreement among raft nodes before linearized reading'  (duration: 274.123429ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-09T06:33:25.766Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-09T06:33:25.195Z","time spent":"570.582188ms","remote":"127.0.0.1:57172","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:45325 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"info","ts":"2023-10-09T06:33:26.634Z","caller":"traceutil/trace.go:171","msg":"trace[1983267673] transaction","detail":"{read_only:false; response_revision:45328; number_of_response:1; }","duration":"236.004557ms","start":"2023-10-09T06:33:26.398Z","end":"2023-10-09T06:33:26.634Z","steps":["trace[1983267673] 'process raft request'  (duration: 235.855536ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-09T06:33:30.497Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"397.815991ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238522520939074534 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr.178c5cd05cca260e\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr.178c5cd05cca260e\" value_size:789 lease:3238522520939074531 >> failure:<>>","response":"size:18"}
{"level":"info","ts":"2023-10-09T06:33:30.497Z","caller":"traceutil/trace.go:171","msg":"trace[485924374] transaction","detail":"{read_only:false; response_revision:45331; number_of_response:1; }","duration":"621.627204ms","start":"2023-10-09T06:33:29.875Z","end":"2023-10-09T06:33:30.497Z","steps":["trace[485924374] 'process raft request'  (duration: 223.679113ms)","trace[485924374] 'compare'  (duration: 397.7041ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-09T06:33:30.497Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-09T06:33:29.875Z","time spent":"621.69833ms","remote":"127.0.0.1:57096","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":896,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/events/ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr.178c5cd05cca260e\" mod_revision:0 > success:<request_put:<key:\"/registry/events/ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr.178c5cd05cca260e\" value_size:789 lease:3238522520939074531 >> failure:<>"}
{"level":"warn","ts":"2023-10-09T06:33:31.222Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"603.933563ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238522520939074535 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:45329 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2023-10-09T06:33:31.222Z","caller":"traceutil/trace.go:171","msg":"trace[1658068632] linearizableReadLoop","detail":"{readStateIndex:57125; appliedIndex:57124; }","duration":"1.073924166s","start":"2023-10-09T06:33:30.148Z","end":"2023-10-09T06:33:31.222Z","steps":["trace[1658068632] 'read index received'  (duration: 469.939033ms)","trace[1658068632] 'applied index is now lower than readState.Index'  (duration: 603.983703ms)"],"step_count":2}
{"level":"info","ts":"2023-10-09T06:33:31.222Z","caller":"traceutil/trace.go:171","msg":"trace[550081872] transaction","detail":"{read_only:false; response_revision:45332; number_of_response:1; }","duration":"1.343126127s","start":"2023-10-09T06:33:29.879Z","end":"2023-10-09T06:33:31.222Z","steps":["trace[550081872] 'process raft request'  (duration: 738.97954ms)","trace[550081872] 'compare'  (duration: 603.598757ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-09T06:33:31.222Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-09T06:33:29.879Z","time spent":"1.343336109s","remote":"127.0.0.1:57172","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:45329 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}
{"level":"warn","ts":"2023-10-09T06:33:31.222Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"1.074073413s","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/health\" ","response":"range_response_count:0 size:6"}
{"level":"info","ts":"2023-10-09T06:33:31.223Z","caller":"traceutil/trace.go:171","msg":"trace[161085991] range","detail":"{range_begin:/registry/health; range_end:; response_count:0; response_revision:45332; }","duration":"1.075400775s","start":"2023-10-09T06:33:30.148Z","end":"2023-10-09T06:33:31.223Z","steps":["trace[161085991] 'agreement among raft nodes before linearized reading'  (duration: 1.073975802s)"],"step_count":1}
{"level":"warn","ts":"2023-10-09T06:33:31.223Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-09T06:33:30.148Z","time spent":"1.075573082s","remote":"127.0.0.1:57054","response type":"/etcdserverpb.KV/Range","request count":0,"request size":18,"response count":0,"response size":29,"request content":"key:\"/registry/health\" "}
{"level":"warn","ts":"2023-10-09T06:33:31.610Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"269.894182ms","expected-duration":"100ms","prefix":"read-only range ","request":"key:\"/registry/controllerrevisions/\" range_end:\"/registry/controllerrevisions0\" count_only:true ","response":"range_response_count:0 size:8"}
{"level":"info","ts":"2023-10-09T06:33:31.610Z","caller":"traceutil/trace.go:171","msg":"trace[1623126188] range","detail":"{range_begin:/registry/controllerrevisions/; range_end:/registry/controllerrevisions0; response_count:0; response_revision:45332; }","duration":"269.963421ms","start":"2023-10-09T06:33:31.340Z","end":"2023-10-09T06:33:31.610Z","steps":["trace[1623126188] 'count revisions from in-memory index tree'  (duration: 269.800721ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:33.377Z","caller":"traceutil/trace.go:171","msg":"trace[2141870151] transaction","detail":"{read_only:false; response_revision:45333; number_of_response:1; }","duration":"148.260906ms","start":"2023-10-09T06:33:33.229Z","end":"2023-10-09T06:33:33.377Z","steps":["trace[2141870151] 'process raft request'  (duration: 148.166678ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:35.756Z","caller":"traceutil/trace.go:171","msg":"trace[173701446] transaction","detail":"{read_only:false; response_revision:45345; number_of_response:1; }","duration":"131.44329ms","start":"2023-10-09T06:33:35.625Z","end":"2023-10-09T06:33:35.756Z","steps":["trace[173701446] 'process raft request'  (duration: 131.397229ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:35.756Z","caller":"traceutil/trace.go:171","msg":"trace[1098021765] transaction","detail":"{read_only:false; response_revision:45344; number_of_response:1; }","duration":"131.61716ms","start":"2023-10-09T06:33:35.625Z","end":"2023-10-09T06:33:35.756Z","steps":["trace[1098021765] 'process raft request'  (duration: 131.380519ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:35.756Z","caller":"traceutil/trace.go:171","msg":"trace[286633008] transaction","detail":"{read_only:false; response_revision:45343; number_of_response:1; }","duration":"131.54322ms","start":"2023-10-09T06:33:35.625Z","end":"2023-10-09T06:33:35.756Z","steps":["trace[286633008] 'process raft request'  (duration: 103.571862ms)","trace[286633008] 'compare'  (duration: 27.825222ms)"],"step_count":2}
{"level":"info","ts":"2023-10-09T06:33:36.754Z","caller":"traceutil/trace.go:171","msg":"trace[893455440] transaction","detail":"{read_only:false; response_revision:45353; number_of_response:1; }","duration":"119.912379ms","start":"2023-10-09T06:33:36.634Z","end":"2023-10-09T06:33:36.754Z","steps":["trace[893455440] 'process raft request'  (duration: 119.878175ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:36.754Z","caller":"traceutil/trace.go:171","msg":"trace[1648742233] transaction","detail":"{read_only:false; response_revision:45352; number_of_response:1; }","duration":"170.961191ms","start":"2023-10-09T06:33:36.583Z","end":"2023-10-09T06:33:36.754Z","steps":["trace[1648742233] 'process raft request'  (duration: 71.465926ms)","trace[1648742233] 'compare'  (duration: 99.360395ms)"],"step_count":2}
{"level":"info","ts":"2023-10-09T06:33:49.012Z","caller":"traceutil/trace.go:171","msg":"trace[228824911] transaction","detail":"{read_only:false; response_revision:45369; number_of_response:1; }","duration":"104.752215ms","start":"2023-10-09T06:33:48.907Z","end":"2023-10-09T06:33:49.012Z","steps":["trace[228824911] 'process raft request'  (duration: 82.162523ms)","trace[228824911] 'compare'  (duration: 22.359323ms)"],"step_count":2}
{"level":"info","ts":"2023-10-09T06:33:49.044Z","caller":"traceutil/trace.go:171","msg":"trace[1477938466] transaction","detail":"{read_only:false; response_revision:45370; number_of_response:1; }","duration":"136.219973ms","start":"2023-10-09T06:33:48.907Z","end":"2023-10-09T06:33:49.044Z","steps":["trace[1477938466] 'process raft request'  (duration: 136.081544ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:49.044Z","caller":"traceutil/trace.go:171","msg":"trace[603153142] transaction","detail":"{read_only:false; response_revision:45371; number_of_response:1; }","duration":"136.199249ms","start":"2023-10-09T06:33:48.907Z","end":"2023-10-09T06:33:49.044Z","steps":["trace[603153142] 'process raft request'  (duration: 136.085837ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:49.044Z","caller":"traceutil/trace.go:171","msg":"trace[805908619] transaction","detail":"{read_only:false; response_revision:45373; number_of_response:1; }","duration":"134.855926ms","start":"2023-10-09T06:33:48.909Z","end":"2023-10-09T06:33:49.044Z","steps":["trace[805908619] 'process raft request'  (duration: 134.591587ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:49.044Z","caller":"traceutil/trace.go:171","msg":"trace[425190239] transaction","detail":"{read_only:false; response_revision:45372; number_of_response:1; }","duration":"136.280816ms","start":"2023-10-09T06:33:48.908Z","end":"2023-10-09T06:33:49.044Z","steps":["trace[425190239] 'process raft request'  (duration: 135.852204ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:49.813Z","caller":"traceutil/trace.go:171","msg":"trace[583723853] transaction","detail":"{read_only:false; response_revision:45375; number_of_response:1; }","duration":"129.21076ms","start":"2023-10-09T06:33:49.684Z","end":"2023-10-09T06:33:49.813Z","steps":["trace[583723853] 'process raft request'  (duration: 129.036333ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:51.712Z","caller":"traceutil/trace.go:171","msg":"trace[547034770] transaction","detail":"{read_only:false; response_revision:45377; number_of_response:1; }","duration":"166.137084ms","start":"2023-10-09T06:33:51.546Z","end":"2023-10-09T06:33:51.712Z","steps":["trace[547034770] 'process raft request'  (duration: 166.009324ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:51.757Z","caller":"traceutil/trace.go:171","msg":"trace[85197876] transaction","detail":"{read_only:false; response_revision:45378; number_of_response:1; }","duration":"208.603005ms","start":"2023-10-09T06:33:51.549Z","end":"2023-10-09T06:33:51.757Z","steps":["trace[85197876] 'process raft request'  (duration: 208.447115ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:54.015Z","caller":"traceutil/trace.go:171","msg":"trace[1283505430] transaction","detail":"{read_only:false; response_revision:45380; number_of_response:1; }","duration":"164.088802ms","start":"2023-10-09T06:33:53.851Z","end":"2023-10-09T06:33:54.015Z","steps":["trace[1283505430] 'process raft request'  (duration: 163.978736ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:55.235Z","caller":"traceutil/trace.go:171","msg":"trace[877668626] transaction","detail":"{read_only:false; response_revision:45381; number_of_response:1; }","duration":"137.861978ms","start":"2023-10-09T06:33:55.097Z","end":"2023-10-09T06:33:55.235Z","steps":["trace[877668626] 'process raft request'  (duration: 137.684417ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:56.146Z","caller":"traceutil/trace.go:171","msg":"trace[1682826175] transaction","detail":"{read_only:false; response_revision:45382; number_of_response:1; }","duration":"125.585601ms","start":"2023-10-09T06:33:56.021Z","end":"2023-10-09T06:33:56.146Z","steps":["trace[1682826175] 'process raft request'  (duration: 125.479999ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:33:56.859Z","caller":"traceutil/trace.go:171","msg":"trace[1880039162] transaction","detail":"{read_only:false; response_revision:45383; number_of_response:1; }","duration":"178.421651ms","start":"2023-10-09T06:33:56.681Z","end":"2023-10-09T06:33:56.859Z","steps":["trace[1880039162] 'process raft request'  (duration: 178.316891ms)"],"step_count":1}
{"level":"warn","ts":"2023-10-09T06:33:57.712Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"164.640838ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238522520939074685 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/events/task-appns/task-app-deployment-5f68b76c59-wj49n.178c5cd1cc5e879a\" mod_revision:45347 > success:<request_put:<key:\"/registry/events/task-appns/task-app-deployment-5f68b76c59-wj49n.178c5cd1cc5e879a\" value_size:595 lease:3238522520939074531 >> failure:<request_range:<key:\"/registry/events/task-appns/task-app-deployment-5f68b76c59-wj49n.178c5cd1cc5e879a\" > >>","response":"size:18"}
{"level":"info","ts":"2023-10-09T06:33:57.713Z","caller":"traceutil/trace.go:171","msg":"trace[924105738] transaction","detail":"{read_only:false; response_revision:45386; number_of_response:1; }","duration":"204.383157ms","start":"2023-10-09T06:33:57.508Z","end":"2023-10-09T06:33:57.712Z","steps":["trace[924105738] 'process raft request'  (duration: 38.85275ms)","trace[924105738] 'compare'  (duration: 164.222689ms)"],"step_count":2}
{"level":"info","ts":"2023-10-09T06:34:02.418Z","caller":"traceutil/trace.go:171","msg":"trace[977025562] transaction","detail":"{read_only:false; response_revision:45391; number_of_response:1; }","duration":"166.700172ms","start":"2023-10-09T06:34:02.251Z","end":"2023-10-09T06:34:02.418Z","steps":["trace[977025562] 'process raft request'  (duration: 166.599455ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:34:06.380Z","caller":"traceutil/trace.go:171","msg":"trace[1273600937] transaction","detail":"{read_only:false; response_revision:45394; number_of_response:1; }","duration":"128.986218ms","start":"2023-10-09T06:34:06.251Z","end":"2023-10-09T06:34:06.380Z","steps":["trace[1273600937] 'process raft request'  (duration: 128.892609ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:34:06.545Z","caller":"traceutil/trace.go:171","msg":"trace[270211589] transaction","detail":"{read_only:false; response_revision:45395; number_of_response:1; }","duration":"104.282584ms","start":"2023-10-09T06:34:06.440Z","end":"2023-10-09T06:34:06.545Z","steps":["trace[270211589] 'process raft request'  (duration: 82.225818ms)","trace[270211589] 'compare'  (duration: 21.980178ms)"],"step_count":2}
{"level":"info","ts":"2023-10-09T06:34:07.480Z","caller":"traceutil/trace.go:171","msg":"trace[1539661897] transaction","detail":"{read_only:false; response_revision:45396; number_of_response:1; }","duration":"148.367566ms","start":"2023-10-09T06:34:07.332Z","end":"2023-10-09T06:34:07.480Z","steps":["trace[1539661897] 'process raft request'  (duration: 148.180893ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:34:08.680Z","caller":"traceutil/trace.go:171","msg":"trace[2070900582] transaction","detail":"{read_only:false; response_revision:45397; number_of_response:1; }","duration":"127.823098ms","start":"2023-10-09T06:34:08.552Z","end":"2023-10-09T06:34:08.680Z","steps":["trace[2070900582] 'process raft request'  (duration: 127.737623ms)"],"step_count":1}
{"level":"info","ts":"2023-10-09T06:34:10.712Z","caller":"traceutil/trace.go:171","msg":"trace[1840321670] transaction","detail":"{read_only:false; response_revision:45400; number_of_response:1; }","duration":"165.86019ms","start":"2023-10-09T06:34:10.546Z","end":"2023-10-09T06:34:10.712Z","steps":["trace[1840321670] 'process raft request'  (duration: 88.862287ms)","trace[1840321670] 'compare'  (duration: 76.829546ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-09T06:34:11.101Z","caller":"etcdserver/util.go:170","msg":"apply request took too long","took":"243.469685ms","expected-duration":"100ms","prefix":"","request":"header:<ID:3238522520939074762 username:\"kube-apiserver-etcd-client\" auth_revision:1 > txn:<compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:45397 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >>","response":"size:18"}
{"level":"info","ts":"2023-10-09T06:34:11.101Z","caller":"traceutil/trace.go:171","msg":"trace[262546149] transaction","detail":"{read_only:false; response_revision:45402; number_of_response:1; }","duration":"385.511849ms","start":"2023-10-09T06:34:10.715Z","end":"2023-10-09T06:34:11.101Z","steps":["trace[262546149] 'process raft request'  (duration: 141.900391ms)","trace[262546149] 'compare'  (duration: 243.386801ms)"],"step_count":2}
{"level":"warn","ts":"2023-10-09T06:34:11.101Z","caller":"v3rpc/interceptor.go:197","msg":"request stats","start time":"2023-10-09T06:34:10.715Z","time spent":"385.583538ms","remote":"127.0.0.1:57172","response type":"/etcdserverpb.KV/Txn","request count":1,"request size":1093,"response count":0,"response size":41,"request content":"compare:<target:MOD key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" mod_revision:45397 > success:<request_put:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" value_size:1020 >> failure:<request_range:<key:\"/registry/services/endpoints/kube-system/k8s.io-minikube-hostpath\" > >"}

* 
* ==> kernel <==
*  06:34:13 up 13:59,  0 users,  load average: 2.30, 2.45, 2.32
Linux minikube 6.2.0-34-generic #34~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Thu Sep  7 13:12:03 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.2 LTS"

* 
* ==> kube-apiserver [2145ab5d4b47] <==
* Trace[1708833716]:  ---"Txn call completed" 775ms (06:29:38.069)]
Trace[1708833716]: [775.964392ms] [775.964392ms] END
I1009 06:29:38.070219       1 trace.go:219] Trace[1385371289]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:9e99606f-59fe-44af-a5e2-8a6873a51427,client:192.168.58.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (09-Oct-2023 06:29:36.793) (total time: 1276ms):
Trace[1385371289]: ["GuaranteedUpdate etcd3" audit-id:9e99606f-59fe-44af-a5e2-8a6873a51427,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 1276ms (06:29:36.793)
Trace[1385371289]:  ---"Txn call completed" 1275ms (06:29:38.070)]
Trace[1385371289]: [1.276704944s] [1.276704944s] END
I1009 06:30:09.340525       1 trace.go:219] Trace[965534429]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (09-Oct-2023 06:30:08.694) (total time: 645ms):
Trace[965534429]: ---"Transaction prepared" 144ms (06:30:08.839)
Trace[965534429]: ---"Txn call completed" 501ms (06:30:09.340)
Trace[965534429]: [645.967756ms] [645.967756ms] END
I1009 06:30:09.341505       1 trace.go:219] Trace[1660293824]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:d60cbf1d-ddda-4a95-8f78-9cf6397ea4f3,client:192.168.58.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (09-Oct-2023 06:30:08.731) (total time: 610ms):
Trace[1660293824]: ["GuaranteedUpdate etcd3" audit-id:d60cbf1d-ddda-4a95-8f78-9cf6397ea4f3,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 610ms (06:30:08.731)
Trace[1660293824]:  ---"Txn call completed" 609ms (06:30:09.341)]
Trace[1660293824]: [610.34795ms] [610.34795ms] END
I1009 06:31:19.431471       1 trace.go:219] Trace[1450788599]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (09-Oct-2023 06:31:18.698) (total time: 732ms):
Trace[1450788599]: ---"Transaction prepared" 448ms (06:31:19.147)
Trace[1450788599]: ---"Txn call completed" 283ms (06:31:19.431)
Trace[1450788599]: [732.799043ms] [732.799043ms] END
I1009 06:31:22.158820       1 trace.go:219] Trace[2064220046]: "Update" accept:application/json, */*,audit-id:5229ffac-c751-497a-8540-c1fc735e4c4a,client:192.168.58.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Oct-2023 06:31:21.435) (total time: 723ms):
Trace[2064220046]: ["GuaranteedUpdate etcd3" audit-id:5229ffac-c751-497a-8540-c1fc735e4c4a,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 723ms (06:31:21.435)
Trace[2064220046]:  ---"Txn call completed" 722ms (06:31:22.158)]
Trace[2064220046]: [723.668918ms] [723.668918ms] END
I1009 06:31:22.251844       1 trace.go:219] Trace[2001334479]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:6dc08789-f668-44fe-a13b-817fbf9dc3c6,client:192.168.58.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (09-Oct-2023 06:31:21.640) (total time: 611ms):
Trace[2001334479]: ["GuaranteedUpdate etcd3" audit-id:6dc08789-f668-44fe-a13b-817fbf9dc3c6,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 610ms (06:31:21.640)
Trace[2001334479]:  ---"Txn call completed" 609ms (06:31:22.251)]
Trace[2001334479]: [611.104148ms] [611.104148ms] END
I1009 06:31:59.740364       1 alloc.go:330] "allocated clusterIPs" service="task-appns/task-app-task-1" clusterIPs=map[IPv4:10.99.199.116]
I1009 06:32:45.122316       1 trace.go:219] Trace[2013912439]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:02c6f39d-e383-4a0a-9072-1b4f1ebdf08d,client:192.168.58.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (09-Oct-2023 06:32:44.508) (total time: 614ms):
Trace[2013912439]: ["GuaranteedUpdate etcd3" audit-id:02c6f39d-e383-4a0a-9072-1b4f1ebdf08d,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 614ms (06:32:44.508)
Trace[2013912439]:  ---"Txn call completed" 613ms (06:32:45.122)]
Trace[2013912439]: [614.151693ms] [614.151693ms] END
I1009 06:33:16.248993       1 trace.go:219] Trace[1415633465]: "Update" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:521727be-076a-4762-83ab-d164b2b6849b,client:192.168.58.2,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (09-Oct-2023 06:33:15.631) (total time: 617ms):
Trace[1415633465]: ["GuaranteedUpdate etcd3" audit-id:521727be-076a-4762-83ab-d164b2b6849b,key:/leases/kube-node-lease/minikube,type:*coordination.Lease,resource:leases.coordination.k8s.io 617ms (06:33:15.631)
Trace[1415633465]:  ---"Txn call completed" 616ms (06:33:16.248)]
Trace[1415633465]: [617.394048ms] [617.394048ms] END
I1009 06:33:19.968720       1 trace.go:219] Trace[2084587030]: "GuaranteedUpdate etcd3" audit-id:,key:/masterleases/192.168.58.2,type:*v1.Endpoints,resource:apiServerIPInfo (09-Oct-2023 06:33:18.705) (total time: 1263ms):
Trace[2084587030]: ---"Transaction prepared" 440ms (06:33:19.146)
Trace[2084587030]: ---"Txn call completed" 821ms (06:33:19.968)
Trace[2084587030]: [1.263204948s] [1.263204948s] END
I1009 06:33:22.804343       1 trace.go:219] Trace[1321590802]: "Update" accept:application/json, */*,audit-id:e8222625-82d0-437b-ac24-f6ec1b2d58a2,client:192.168.58.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Oct-2023 06:33:22.061) (total time: 742ms):
Trace[1321590802]: ["GuaranteedUpdate etcd3" audit-id:e8222625-82d0-437b-ac24-f6ec1b2d58a2,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 742ms (06:33:22.061)
Trace[1321590802]:  ---"Txn call completed" 741ms (06:33:22.804)]
Trace[1321590802]: [742.600962ms] [742.600962ms] END
I1009 06:33:24.289268       1 trace.go:219] Trace[2084477345]: "Update" accept:application/vnd.kubernetes.protobuf, */*,audit-id:f70ae0be-741e-4481-ae48-2ed3a3a6c790,client:127.0.0.1,protocol:HTTP/2.0,resource:leases,scope:resource,url:/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,user-agent:kube-apiserver/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:PUT (09-Oct-2023 06:33:23.731) (total time: 557ms):
Trace[2084477345]: ["GuaranteedUpdate etcd3" audit-id:f70ae0be-741e-4481-ae48-2ed3a3a6c790,key:/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii,type:*coordination.Lease,resource:leases.coordination.k8s.io 557ms (06:33:23.731)
Trace[2084477345]:  ---"Txn call completed" 557ms (06:33:24.289)]
Trace[2084477345]: [557.759234ms] [557.759234ms] END
I1009 06:33:25.767073       1 trace.go:219] Trace[2034287585]: "Update" accept:application/json, */*,audit-id:16a33f84-2354-49fd-8435-091f4b27cc97,client:192.168.58.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Oct-2023 06:33:25.194) (total time: 572ms):
Trace[2034287585]: ["GuaranteedUpdate etcd3" audit-id:16a33f84-2354-49fd-8435-091f4b27cc97,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 572ms (06:33:25.194)
Trace[2034287585]:  ---"Txn call completed" 571ms (06:33:25.766)]
Trace[2034287585]: [572.663869ms] [572.663869ms] END
I1009 06:33:30.498187       1 trace.go:219] Trace[1837655597]: "Create" accept:application/vnd.kubernetes.protobuf,application/json,audit-id:0a3a23b0-8904-4960-890e-3e85cc0c85aa,client:192.168.58.2,protocol:HTTP/2.0,resource:events,scope:resource,url:/api/v1/namespaces/ingress-nginx/events,user-agent:kubelet/v1.27.4 (linux/amd64) kubernetes/fa3d799,verb:POST (09-Oct-2023 06:33:29.592) (total time: 905ms):
Trace[1837655597]: ["Create etcd3" audit-id:0a3a23b0-8904-4960-890e-3e85cc0c85aa,key:/events/ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr.178c5cd05cca260e,type:*core.Event,resource:events 904ms (06:33:29.593)
Trace[1837655597]:  ---"TransformToStorage succeeded" 281ms (06:33:29.875)
Trace[1837655597]:  ---"Txn call succeeded" 622ms (06:33:30.497)]
Trace[1837655597]: [905.236216ms] [905.236216ms] END
I1009 06:33:31.223687       1 trace.go:219] Trace[595924722]: "Update" accept:application/json, */*,audit-id:52e8b613-b73f-40ff-a208-aa6969c9f734,client:192.168.58.2,protocol:HTTP/2.0,resource:endpoints,scope:resource,url:/api/v1/namespaces/kube-system/endpoints/k8s.io-minikube-hostpath,user-agent:storage-provisioner/v0.0.0 (linux/amd64) kubernetes/$Format,verb:PUT (09-Oct-2023 06:33:29.878) (total time: 1345ms):
Trace[595924722]: ["GuaranteedUpdate etcd3" audit-id:52e8b613-b73f-40ff-a208-aa6969c9f734,key:/services/endpoints/kube-system/k8s.io-minikube-hostpath,type:*core.Endpoints,resource:endpoints 1345ms (06:33:29.878)
Trace[595924722]:  ---"Txn call completed" 1344ms (06:33:31.223)]
Trace[595924722]: [1.345485435s] [1.345485435s] END

* 
* ==> kube-apiserver [a099c60c8a87] <==
*   "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0904 20:05:13.004335       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
W0904 20:05:13.004444       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {
  "Addr": "127.0.0.1:2379",
  "ServerName": "127.0.0.1",
  "Attributes": null,
  "BalancerAttributes": null,
  "Type": 0,
  "Metadata": null
}. Err: connection error: desc = "transport: Error while dialing dial tcp 127.0.0.1:2379: connect: connection refused"
E0904 20:05:13.004513       1 controller.go:231] Unable to remove endpoints from kubernetes service: rpc error: code = Unknown desc = malformed header: missing HTTP content-type
I0904 20:05:13.004623       1 dynamic_serving_content.go:146] "Shutting down controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0904 20:05:13.005468       1 storage_flowcontrol.go:179] APF bootstrap ensurer is exiting
I0904 20:05:13.005485       1 cluster_authentication_trust_controller.go:463] Shutting down cluster_authentication_trust_controller controller
I0904 20:05:13.005636       1 object_count_tracker.go:151] "StorageObjectCountTracker pruner is exiting"
I0904 20:05:13.005649       1 system_namespaces_controller.go:77] Shutting down system namespaces controller
I0904 20:05:13.005659       1 apiservice_controller.go:131] Shutting down APIServiceRegistrationController
I0904 20:05:13.005671       1 autoregister_controller.go:165] Shutting down autoregister controller
I0904 20:05:13.005684       1 crdregistration_controller.go:142] Shutting down crd-autoregister controller
I0904 20:05:13.005695       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I0904 20:05:13.005705       1 apf_controller.go:373] Shutting down API Priority and Fairness config worker
I0904 20:05:13.005729       1 controller.go:134] Ending legacy_token_tracking_controller
I0904 20:05:13.005735       1 controller.go:135] Shutting down legacy_token_tracking_controller
I0904 20:05:13.005745       1 gc_controller.go:91] Shutting down apiserver lease garbage collector
I0904 20:05:13.005755       1 available_controller.go:439] Shutting down AvailableConditionController
I0904 20:05:13.005769       1 controller.go:115] Shutting down OpenAPI V3 controller
I0904 20:05:13.005783       1 controller.go:122] Shutting down OpenAPI controller
I0904 20:05:13.005794       1 apiapproval_controller.go:198] Shutting down KubernetesAPIApprovalPolicyConformantConditionController
I0904 20:05:13.005808       1 nonstructuralschema_controller.go:204] Shutting down NonStructuralSchemaConditionController
I0904 20:05:13.005822       1 establishing_controller.go:87] Shutting down EstablishingController
I0904 20:05:13.005833       1 naming_controller.go:302] Shutting down NamingConditionController
I0904 20:05:13.005844       1 crd_finalizer.go:278] Shutting down CRDFinalizer
I0904 20:05:13.005857       1 customresource_discovery_controller.go:325] Shutting down DiscoveryController
I0904 20:05:13.011419       1 dynamic_cafile_content.go:171] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0904 20:05:13.012051       1 dynamic_cafile_content.go:171] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0904 20:05:13.012083       1 controller.go:89] Shutting down OpenAPI AggregationController
I0904 20:05:13.012364       1 controller.go:86] Shutting down OpenAPI V3 AggregationController
I0904 20:05:13.012391       1 dynamic_cafile_content.go:171] "Shutting down controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0904 20:05:13.012432       1 controller.go:159] Shutting down quota evaluator
I0904 20:05:13.012445       1 controller.go:178] quota evaluator worker shutdown
I0904 20:05:13.012550       1 controller.go:178] quota evaluator worker shutdown
I0904 20:05:13.012560       1 controller.go:178] quota evaluator worker shutdown
I0904 20:05:13.012566       1 controller.go:178] quota evaluator worker shutdown
I0904 20:05:13.012572       1 controller.go:178] quota evaluator worker shutdown
I0904 20:05:13.012636       1 dynamic_serving_content.go:146] "Shutting down controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0904 20:05:13.012693       1 secure_serving.go:255] Stopped listening on [::]:8443
I0904 20:05:13.012707       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0904 20:05:13.012985       1 dynamic_cafile_content.go:171] "Shutting down controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"

* 
* ==> kube-controller-manager [2a7f2491e1e4] <==
* E0904 17:14:05.566052       1 resource_quota_controller.go:441] failed to discover resources: the server has asked for the client to provide credentials
W0904 17:14:06.004041       1 garbagecollector.go:818] failed to discover preferred resources: the server has asked for the client to provide credentials
I0904 19:29:05.445956       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:29:05.509594       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set ingress-nginx-controller-7799c6795f to 1"
I0904 19:29:05.760109       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:29:05.820938       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:29:05.821204       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-create-g7p8h"
I0904 19:29:05.946605       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:29:05.948160       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-admission-patch-5kwk7"
I0904 19:29:05.979447       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:29:05.981722       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:29:06.039670       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:29:06.041008       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:29:06.043581       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-controller-7799c6795f" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: ingress-nginx-controller-7799c6795f-mvzvr"
I0904 19:29:06.244393       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:29:06.915192       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:29:13.974297       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:29:14.266451       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:29:14.985347       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:29:15.207584       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:29:37.114078       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:29:42.248184       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:29:52.319476       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:29:54.231025       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:30:06.241799       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:30:07.133200       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:30:19.201927       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:30:20.216615       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:30:45.320853       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:30:56.162102       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:30:57.051571       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:31:08.204141       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:32:14.330960       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:32:28.093850       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:32:32.161663       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:32:43.126724       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:35:01.063551       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:35:13.040053       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:35:14.062731       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:35:26.156692       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:40:12.135110       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:40:23.069368       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:40:23.191796       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:40:35.186723       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:45:13.046985       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:45:26.074081       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:45:34.231005       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:45:47.220291       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:50:30.161692       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:50:40.165797       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:50:41.177067       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:50:51.081225       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:55:34.205524       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:55:39.186651       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 19:55:45.055961       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 19:55:54.060684       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 20:00:44.058741       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 20:00:48.092752       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I0904 20:00:59.052864       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I0904 20:01:03.077198       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch

* 
* ==> kube-controller-manager [77d78ae6608a] <==
* I1009 06:05:39.275900       1 shared_informer.go:318] Caches are synced for daemon sets
I1009 06:05:39.275957       1 shared_informer.go:318] Caches are synced for attach detach
I1009 06:05:39.294021       1 shared_informer.go:318] Caches are synced for deployment
I1009 06:05:39.382008       1 shared_informer.go:318] Caches are synced for disruption
I1009 06:05:39.382779       1 shared_informer.go:318] Caches are synced for resource quota
I1009 06:05:39.382944       1 shared_informer.go:318] Caches are synced for endpoint
I1009 06:05:39.383190       1 shared_informer.go:318] Caches are synced for stateful set
I1009 06:05:39.383393       1 shared_informer.go:318] Caches are synced for TTL
I1009 06:05:39.383614       1 shared_informer.go:318] Caches are synced for HPA
I1009 06:05:39.383893       1 shared_informer.go:318] Caches are synced for node
I1009 06:05:39.384276       1 range_allocator.go:174] "Sending events to api server"
I1009 06:05:39.384552       1 range_allocator.go:178] "Starting range CIDR allocator"
I1009 06:05:39.384696       1 shared_informer.go:311] Waiting for caches to sync for cidrallocator
I1009 06:05:39.384972       1 shared_informer.go:318] Caches are synced for cidrallocator
I1009 06:05:39.385200       1 shared_informer.go:318] Caches are synced for endpoint_slice_mirroring
I1009 06:05:39.385878       1 shared_informer.go:318] Caches are synced for ephemeral
I1009 06:05:39.382008       1 shared_informer.go:318] Caches are synced for taint
I1009 06:05:39.386610       1 node_lifecycle_controller.go:1223] "Initializing eviction metric for zone" zone=""
I1009 06:05:39.387019       1 node_lifecycle_controller.go:875] "Missing timestamp for Node. Assuming now as a timestamp" node="minikube"
I1009 06:05:39.387148       1 node_lifecycle_controller.go:1069] "Controller detected that zone is now in new state" zone="" newState=Normal
I1009 06:05:39.382019       1 shared_informer.go:318] Caches are synced for job
I1009 06:05:39.469743       1 taint_manager.go:206] "Starting NoExecuteTaintManager"
I1009 06:05:39.469837       1 taint_manager.go:211] "Sending events to api server"
I1009 06:05:39.470225       1 event.go:307] "Event occurred" object="minikube" fieldPath="" kind="Node" apiVersion="v1" type="Normal" reason="RegisteredNode" message="Node minikube event: Registered Node minikube in Controller"
I1009 06:05:39.382236       1 shared_informer.go:318] Caches are synced for resource quota
I1009 06:05:39.382330       1 shared_informer.go:318] Caches are synced for ReplicationController
I1009 06:05:39.383197       1 shared_informer.go:318] Caches are synced for persistent volume
I1009 06:05:39.382448       1 shared_informer.go:318] Caches are synced for endpoint_slice
I1009 06:05:39.382414       1 shared_informer.go:318] Caches are synced for GC
I1009 06:05:39.633200       1 shared_informer.go:318] Caches are synced for garbage collector
I1009 06:05:39.656253       1 shared_informer.go:318] Caches are synced for garbage collector
I1009 06:05:39.656270       1 garbagecollector.go:166] "All resource monitors have synced. Proceeding to collect garbage"
I1009 06:11:45.268727       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:11:55.789449       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:11:56.508731       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:12:12.964511       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:12:13.841317       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:12:44.911203       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:12:45.717963       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:13:28.352528       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:13:29.391207       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:14:56.192898       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:15:07.770489       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:17:44.003699       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:17:45.060939       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:17:45.061368       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="SuccessfulDelete" message="Deleted pod: ingress-nginx-admission-patch-5kwk7"
I1009 06:17:45.127818       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:17:45.277520       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:17:45.475351       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:17:45.563932       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-patch
I1009 06:17:45.564100       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-patch" fieldPath="" kind="Job" apiVersion="batch/v1" type="Warning" reason="BackoffLimitExceeded" message="Job has reached the specified backoff limit"
I1009 06:24:18.185319       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1009 06:24:21.164801       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1009 06:24:22.099979       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1009 06:24:22.243924       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1009 06:24:22.441087       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1009 06:24:22.606280       1 job_controller.go:523] enqueueing job ingress-nginx/ingress-nginx-admission-create
I1009 06:24:22.608467       1 event.go:307] "Event occurred" object="ingress-nginx/ingress-nginx-admission-create" fieldPath="" kind="Job" apiVersion="batch/v1" type="Normal" reason="Completed" message="Job completed"
I1009 06:29:35.393078       1 event.go:307] "Event occurred" object="task-appns/task-app-deployment" fieldPath="" kind="Deployment" apiVersion="apps/v1" type="Normal" reason="ScalingReplicaSet" message="Scaled up replica set task-app-deployment-5f68b76c59 to 1"
I1009 06:29:35.478149       1 event.go:307] "Event occurred" object="task-appns/task-app-deployment-5f68b76c59" fieldPath="" kind="ReplicaSet" apiVersion="apps/v1" type="Normal" reason="SuccessfulCreate" message="Created pod: task-app-deployment-5f68b76c59-wj49n"

* 
* ==> kube-proxy [43db99ca2815] <==
* I0904 09:24:10.326178       1 node.go:141] Successfully retrieved node IP: 192.168.58.2
I0904 09:24:10.326298       1 server_others.go:110] "Detected node IP" address="192.168.58.2"
I0904 09:24:10.326336       1 server_others.go:554] "Using iptables proxy"
I0904 09:24:11.876956       1 server_others.go:192] "Using iptables Proxier"
I0904 09:24:11.876993       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I0904 09:24:11.877008       1 server_others.go:200] "Creating dualStackProxier for iptables"
I0904 09:24:11.877030       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I0904 09:24:11.951869       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0904 09:24:11.952920       1 server.go:658] "Version info" version="v1.27.4"
I0904 09:24:11.953168       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0904 09:24:12.033371       1 config.go:188] "Starting service config controller"
I0904 09:24:12.033635       1 shared_informer.go:311] Waiting for caches to sync for service config
I0904 09:24:12.033374       1 config.go:97] "Starting endpoint slice config controller"
I0904 09:24:12.033783       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I0904 09:24:12.033891       1 config.go:315] "Starting node config controller"
I0904 09:24:12.033922       1 shared_informer.go:311] Waiting for caches to sync for node config
I0904 09:24:12.134392       1 shared_informer.go:318] Caches are synced for node config
I0904 09:24:12.134406       1 shared_informer.go:318] Caches are synced for service config
I0904 09:24:12.134435       1 shared_informer.go:318] Caches are synced for endpoint slice config

* 
* ==> kube-proxy [9f7c9e5a19c9] <==
* I1009 06:06:33.243090       1 node.go:141] Successfully retrieved node IP: 192.168.58.2
I1009 06:06:33.243181       1 server_others.go:110] "Detected node IP" address="192.168.58.2"
I1009 06:06:33.243229       1 server_others.go:554] "Using iptables proxy"
I1009 06:06:36.295587       1 server_others.go:192] "Using iptables Proxier"
I1009 06:06:36.295638       1 server_others.go:199] "kube-proxy running in dual-stack mode" ipFamily=IPv4
I1009 06:06:36.295655       1 server_others.go:200] "Creating dualStackProxier for iptables"
I1009 06:06:36.295678       1 server_others.go:484] "Detect-local-mode set to ClusterCIDR, but no IPv6 cluster CIDR defined, defaulting to no-op detect-local for IPv6"
I1009 06:06:36.295786       1 proxier.go:253] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I1009 06:06:36.296628       1 server.go:658] "Version info" version="v1.27.4"
I1009 06:06:36.296647       1 server.go:660] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I1009 06:06:37.068646       1 config.go:315] "Starting node config controller"
I1009 06:06:37.068677       1 config.go:97] "Starting endpoint slice config controller"
I1009 06:06:37.068685       1 shared_informer.go:311] Waiting for caches to sync for node config
I1009 06:06:37.068653       1 config.go:188] "Starting service config controller"
I1009 06:06:37.068896       1 shared_informer.go:311] Waiting for caches to sync for service config
I1009 06:06:37.068687       1 shared_informer.go:311] Waiting for caches to sync for endpoint slice config
I1009 06:06:37.169670       1 shared_informer.go:318] Caches are synced for endpoint slice config
I1009 06:06:37.169688       1 shared_informer.go:318] Caches are synced for node config
I1009 06:06:37.169695       1 shared_informer.go:318] Caches are synced for service config

* 
* ==> kube-scheduler [ddeb5f0b7021] <==
* E1009 06:04:51.411829       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:51.468386       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:51.468436       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:54.625312       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:54.625364       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:54.761030       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:54.761170       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:54.905378       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:54.905522       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:55.001620       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:55.001704       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:55.014947       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:55.014999       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:55.297837       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:55.297888       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:55.604178       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:55.604401       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:55.626749       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:55.626808       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:55.682954       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:55.683107       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:56.036297       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:56.036347       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:56.103628       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:56.103759       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:56.170348       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:56.170404       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:56.200808       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:56.201002       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:56.685143       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:56.685349       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:04:56.752577       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:04:56.752702       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:05:02.169512       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:05:02.169555       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:05:02.189690       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:05:02.189799       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:05:02.629860       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E1009 06:05:02.629892       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W1009 06:05:08.593961       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1009 06:05:08.594735       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
W1009 06:05:08.594353       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W1009 06:05:08.594445       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E1009 06:05:08.595211       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W1009 06:05:08.594523       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1009 06:05:08.595508       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W1009 06:05:08.594582       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1009 06:05:08.595760       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W1009 06:05:08.594603       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W1009 06:05:08.594623       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W1009 06:05:08.594634       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
W1009 06:05:08.594701       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1009 06:05:08.595576       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E1009 06:05:08.596050       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E1009 06:05:08.596184       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1009 06:05:08.596318       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: csinodes.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csinodes" in API group "storage.k8s.io" at the cluster scope
E1009 06:05:08.596450       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
W1009 06:05:08.655700       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E1009 06:05:08.655733       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I1009 06:05:22.216554       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file

* 
* ==> kube-scheduler [eac185715e7f] <==
* E0904 09:23:07.059216       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:07.092178       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:07.092217       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:07.133851       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:07.134056       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:07.243177       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:07.243218       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:08.733681       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:08.733722       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: Get "https://192.168.58.2:8443/api/v1/persistentvolumeclaims?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:08.756405       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: Get "https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:08.756465       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: Get "https://192.168.58.2:8443/api/v1/namespaces/kube-system/configmaps?fieldSelector=metadata.name%!D(MISSING)extension-apiserver-authentication&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:08.797260       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:08.797299       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://192.168.58.2:8443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:08.813624       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:08.813655       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://192.168.58.2:8443/api/v1/services?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:09.202700       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:09.202735       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csistoragecapacities?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:09.237147       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:09.237182       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSINode: failed to list *v1.CSINode: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csinodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:09.335234       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:09.335269       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: Get "https://192.168.58.2:8443/apis/apps/v1/statefulsets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:09.577735       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:09.577792       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Pod: failed to list *v1.Pod: Get "https://192.168.58.2:8443/api/v1/pods?fieldSelector=status.phase%3DSucceeded%!C(MISSING)status.phase%3DFailed&limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:09.607050       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:09.607081       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: Get "https://192.168.58.2:8443/apis/apps/v1/replicasets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:09.700246       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:09.700294       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://192.168.58.2:8443/api/v1/nodes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:09.700246       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:09.700338       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolume: failed to list *v1.PersistentVolume: Get "https://192.168.58.2:8443/api/v1/persistentvolumes?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:09.867629       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:09.867660       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PodDisruptionBudget: failed to list *v1.PodDisruptionBudget: Get "https://192.168.58.2:8443/apis/policy/v1/poddisruptionbudgets?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:09.938229       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:09.938261       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicationController: failed to list *v1.ReplicationController: Get "https://192.168.58.2:8443/api/v1/replicationcontrollers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:10.031009       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:10.031087       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:10.133062       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
E0904 09:23:10.133182       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: Get "https://192.168.58.2:8443/apis/storage.k8s.io/v1/storageclasses?limit=500&resourceVersion=0": dial tcp 192.168.58.2:8443: connect: connection refused
W0904 09:23:15.035074       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E0904 09:23:15.035153       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StatefulSet: failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
W0904 09:23:15.035213       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E0904 09:23:15.035240       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Service: failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
W0904 09:23:15.035376       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
E0904 09:23:15.035406       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.StorageClass: failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
W0904 09:23:15.035521       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E0904 09:23:15.035549       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.PersistentVolumeClaim: failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
W0904 09:23:15.035648       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
E0904 09:23:15.035672       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Namespace: failed to list *v1.Namespace: namespaces is forbidden: User "system:kube-scheduler" cannot list resource "namespaces" in API group "" at the cluster scope
W0904 09:23:15.035750       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
E0904 09:23:15.035772       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.CSIStorageCapacity: failed to list *v1.CSIStorageCapacity: csistoragecapacities.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "csistoragecapacities" in API group "storage.k8s.io" at the cluster scope
W0904 09:23:15.035897       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E0904 09:23:15.035920       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.ReplicaSet: failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
W0904 09:23:15.035974       1 reflector.go:533] vendor/k8s.io/client-go/informers/factory.go:150: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E0904 09:23:15.035998       1 reflector.go:148] vendor/k8s.io/client-go/informers/factory.go:150: Failed to watch *v1.Node: failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
W0904 09:23:15.063567       1 reflector.go:533] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
E0904 09:23:15.063644       1 reflector.go:148] pkg/server/dynamiccertificates/configmap_cafile_content.go:206: Failed to watch *v1.ConfigMap: failed to list *v1.ConfigMap: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot list resource "configmaps" in API group "" in the namespace "kube-system"
I0904 09:23:27.274538       1 shared_informer.go:318] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0904 20:05:12.949291       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0904 20:05:12.949470       1 secure_serving.go:255] Stopped listening on 127.0.0.1:10259
I0904 20:05:12.950332       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
E0904 20:05:12.950557       1 run.go:74] "command failed" err="finished without leader elect"

* 
* ==> kubelet <==
* Oct 09 06:17:02 minikube kubelet[1367]: I1009 06:17:02.539690    1367 scope.go:115] "RemoveContainer" containerID="6f967c5ff71209ce4b2d1002eb96d9128d165b7611ad8d31658ed1e403059894"
Oct 09 06:17:02 minikube kubelet[1367]: E1009 06:17:02.539934    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=patch pod=ingress-nginx-admission-patch-5kwk7_ingress-nginx(6621fef7-cc9d-4b80-b0d2-27a9ceea0279)\"" pod="ingress-nginx/ingress-nginx-admission-patch-5kwk7" podUID=6621fef7-cc9d-4b80-b0d2-27a9ceea0279
Oct 09 06:17:14 minikube kubelet[1367]: I1009 06:17:14.539382    1367 scope.go:115] "RemoveContainer" containerID="6f967c5ff71209ce4b2d1002eb96d9128d165b7611ad8d31658ed1e403059894"
Oct 09 06:17:14 minikube kubelet[1367]: E1009 06:17:14.539635    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=patch pod=ingress-nginx-admission-patch-5kwk7_ingress-nginx(6621fef7-cc9d-4b80-b0d2-27a9ceea0279)\"" pod="ingress-nginx/ingress-nginx-admission-patch-5kwk7" podUID=6621fef7-cc9d-4b80-b0d2-27a9ceea0279
Oct 09 06:17:29 minikube kubelet[1367]: I1009 06:17:29.539487    1367 scope.go:115] "RemoveContainer" containerID="6f967c5ff71209ce4b2d1002eb96d9128d165b7611ad8d31658ed1e403059894"
Oct 09 06:17:29 minikube kubelet[1367]: E1009 06:17:29.539952    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with CrashLoopBackOff: \"back-off 2m40s restarting failed container=patch pod=ingress-nginx-admission-patch-5kwk7_ingress-nginx(6621fef7-cc9d-4b80-b0d2-27a9ceea0279)\"" pod="ingress-nginx/ingress-nginx-admission-patch-5kwk7" podUID=6621fef7-cc9d-4b80-b0d2-27a9ceea0279
Oct 09 06:17:40 minikube kubelet[1367]: I1009 06:17:40.539431    1367 scope.go:115] "RemoveContainer" containerID="6f967c5ff71209ce4b2d1002eb96d9128d165b7611ad8d31658ed1e403059894"
Oct 09 06:17:43 minikube kubelet[1367]: I1009 06:17:43.879881    1367 scope.go:115] "RemoveContainer" containerID="6f967c5ff71209ce4b2d1002eb96d9128d165b7611ad8d31658ed1e403059894"
Oct 09 06:17:43 minikube kubelet[1367]: I1009 06:17:43.881222    1367 scope.go:115] "RemoveContainer" containerID="5e1cfdb9d8cac5f4d6f63d6825173621d6cc151c5c661c7e8aa514e6984413a3"
Oct 09 06:17:43 minikube kubelet[1367]: E1009 06:17:43.881609    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"patch\" with CrashLoopBackOff: \"back-off 5m0s restarting failed container=patch pod=ingress-nginx-admission-patch-5kwk7_ingress-nginx(6621fef7-cc9d-4b80-b0d2-27a9ceea0279)\"" pod="ingress-nginx/ingress-nginx-admission-patch-5kwk7" podUID=6621fef7-cc9d-4b80-b0d2-27a9ceea0279
Oct 09 06:17:46 minikube kubelet[1367]: I1009 06:17:46.918167    1367 scope.go:115] "RemoveContainer" containerID="5e1cfdb9d8cac5f4d6f63d6825173621d6cc151c5c661c7e8aa514e6984413a3"
Oct 09 06:17:47 minikube kubelet[1367]: I1009 06:17:47.026982    1367 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-zg25m\" (UniqueName: \"kubernetes.io/projected/6621fef7-cc9d-4b80-b0d2-27a9ceea0279-kube-api-access-zg25m\") pod \"6621fef7-cc9d-4b80-b0d2-27a9ceea0279\" (UID: \"6621fef7-cc9d-4b80-b0d2-27a9ceea0279\") "
Oct 09 06:17:47 minikube kubelet[1367]: I1009 06:17:47.155384    1367 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/6621fef7-cc9d-4b80-b0d2-27a9ceea0279-kube-api-access-zg25m" (OuterVolumeSpecName: "kube-api-access-zg25m") pod "6621fef7-cc9d-4b80-b0d2-27a9ceea0279" (UID: "6621fef7-cc9d-4b80-b0d2-27a9ceea0279"). InnerVolumeSpecName "kube-api-access-zg25m". PluginName "kubernetes.io/projected", VolumeGidValue ""
Oct 09 06:17:47 minikube kubelet[1367]: I1009 06:17:47.228246    1367 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-zg25m\" (UniqueName: \"kubernetes.io/projected/6621fef7-cc9d-4b80-b0d2-27a9ceea0279-kube-api-access-zg25m\") on node \"minikube\" DevicePath \"\""
Oct 09 06:17:47 minikube kubelet[1367]: I1009 06:17:47.545582    1367 kubelet_volumes.go:161] "Cleaned up orphaned pod volumes dir" podUID=6621fef7-cc9d-4b80-b0d2-27a9ceea0279 path="/var/lib/kubelet/pods/6621fef7-cc9d-4b80-b0d2-27a9ceea0279/volumes"
Oct 09 06:17:50 minikube kubelet[1367]: E1009 06:17:50.345631    1367 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 09 06:17:50 minikube kubelet[1367]: E1009 06:17:50.345693    1367 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/82b28371-b003-43cd-93c9-a4351c99310d-webhook-cert podName:82b28371-b003-43cd-93c9-a4351c99310d nodeName:}" failed. No retries permitted until 2023-10-09 06:19:52.345674993 +0000 UTC m=+962.867796103 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/82b28371-b003-43cd-93c9-a4351c99310d-webhook-cert") pod "ingress-nginx-controller-7799c6795f-mvzvr" (UID: "82b28371-b003-43cd-93c9-a4351c99310d") : secret "ingress-nginx-admission" not found
Oct 09 06:18:56 minikube kubelet[1367]: E1009 06:18:56.539980    1367 kubelet.go:1875] "Unable to attach or mount volumes for pod; skipping pod" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr"
Oct 09 06:18:56 minikube kubelet[1367]: E1009 06:18:56.540017    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr" podUID=82b28371-b003-43cd-93c9-a4351c99310d
Oct 09 06:19:52 minikube kubelet[1367]: E1009 06:19:52.405868    1367 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 09 06:19:52 minikube kubelet[1367]: E1009 06:19:52.405984    1367 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/82b28371-b003-43cd-93c9-a4351c99310d-webhook-cert podName:82b28371-b003-43cd-93c9-a4351c99310d nodeName:}" failed. No retries permitted until 2023-10-09 06:21:54.405952801 +0000 UTC m=+1084.928073930 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/82b28371-b003-43cd-93c9-a4351c99310d-webhook-cert") pod "ingress-nginx-controller-7799c6795f-mvzvr" (UID: "82b28371-b003-43cd-93c9-a4351c99310d") : secret "ingress-nginx-admission" not found
Oct 09 06:21:10 minikube kubelet[1367]: E1009 06:21:10.539648    1367 kubelet.go:1875] "Unable to attach or mount volumes for pod; skipping pod" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr"
Oct 09 06:21:10 minikube kubelet[1367]: E1009 06:21:10.539679    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr" podUID=82b28371-b003-43cd-93c9-a4351c99310d
Oct 09 06:21:54 minikube kubelet[1367]: E1009 06:21:54.432117    1367 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 09 06:21:54 minikube kubelet[1367]: E1009 06:21:54.432181    1367 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/82b28371-b003-43cd-93c9-a4351c99310d-webhook-cert podName:82b28371-b003-43cd-93c9-a4351c99310d nodeName:}" failed. No retries permitted until 2023-10-09 06:23:56.432165579 +0000 UTC m=+1206.954286700 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/82b28371-b003-43cd-93c9-a4351c99310d-webhook-cert") pod "ingress-nginx-controller-7799c6795f-mvzvr" (UID: "82b28371-b003-43cd-93c9-a4351c99310d") : secret "ingress-nginx-admission" not found
Oct 09 06:23:24 minikube kubelet[1367]: E1009 06:23:24.539833    1367 kubelet.go:1875] "Unable to attach or mount volumes for pod; skipping pod" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr"
Oct 09 06:23:24 minikube kubelet[1367]: E1009 06:23:24.539865    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr" podUID=82b28371-b003-43cd-93c9-a4351c99310d
Oct 09 06:23:56 minikube kubelet[1367]: E1009 06:23:56.521931    1367 secret.go:194] Couldn't get secret ingress-nginx/ingress-nginx-admission: secret "ingress-nginx-admission" not found
Oct 09 06:23:56 minikube kubelet[1367]: E1009 06:23:56.522001    1367 nestedpendingoperations.go:348] Operation for "{volumeName:kubernetes.io/secret/82b28371-b003-43cd-93c9-a4351c99310d-webhook-cert podName:82b28371-b003-43cd-93c9-a4351c99310d nodeName:}" failed. No retries permitted until 2023-10-09 06:25:58.521982307 +0000 UTC m=+1329.044103433 (durationBeforeRetry 2m2s). Error: MountVolume.SetUp failed for volume "webhook-cert" (UniqueName: "kubernetes.io/secret/82b28371-b003-43cd-93c9-a4351c99310d-webhook-cert") pod "ingress-nginx-controller-7799c6795f-mvzvr" (UID: "82b28371-b003-43cd-93c9-a4351c99310d") : secret "ingress-nginx-admission" not found
Oct 09 06:24:21 minikube kubelet[1367]: I1009 06:24:21.267862    1367 reconciler_common.go:172] "operationExecutor.UnmountVolume started for volume \"kube-api-access-wlrdc\" (UniqueName: \"kubernetes.io/projected/857c258a-1ebb-4ad2-abe9-b8c24b334e5e-kube-api-access-wlrdc\") pod \"857c258a-1ebb-4ad2-abe9-b8c24b334e5e\" (UID: \"857c258a-1ebb-4ad2-abe9-b8c24b334e5e\") "
Oct 09 06:24:21 minikube kubelet[1367]: I1009 06:24:21.393916    1367 operation_generator.go:878] UnmountVolume.TearDown succeeded for volume "kubernetes.io/projected/857c258a-1ebb-4ad2-abe9-b8c24b334e5e-kube-api-access-wlrdc" (OuterVolumeSpecName: "kube-api-access-wlrdc") pod "857c258a-1ebb-4ad2-abe9-b8c24b334e5e" (UID: "857c258a-1ebb-4ad2-abe9-b8c24b334e5e"). InnerVolumeSpecName "kube-api-access-wlrdc". PluginName "kubernetes.io/projected", VolumeGidValue ""
Oct 09 06:24:21 minikube kubelet[1367]: I1009 06:24:21.469761    1367 reconciler_common.go:300] "Volume detached for volume \"kube-api-access-wlrdc\" (UniqueName: \"kubernetes.io/projected/857c258a-1ebb-4ad2-abe9-b8c24b334e5e-kube-api-access-wlrdc\") on node \"minikube\" DevicePath \"\""
Oct 09 06:24:22 minikube kubelet[1367]: I1009 06:24:22.020307    1367 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="887cbf22619bf8395011c49490699758ee6cb9509f8d8f655e0fc6ef72b9172d"
Oct 09 06:25:41 minikube kubelet[1367]: E1009 06:25:41.540144    1367 kubelet.go:1875] "Unable to attach or mount volumes for pod; skipping pod" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr"
Oct 09 06:25:41 minikube kubelet[1367]: E1009 06:25:41.540188    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="unmounted volumes=[webhook-cert], unattached volumes=[], failed to process volumes=[]: timed out waiting for the condition" pod="ingress-nginx/ingress-nginx-controller-7799c6795f-mvzvr" podUID=82b28371-b003-43cd-93c9-a4351c99310d
Oct 09 06:26:04 minikube kubelet[1367]: I1009 06:26:04.184380    1367 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5f549f8991a1002049bbf3b08847294ef624e9b08b99acedc9ac83dccbf46b8b"
Oct 09 06:29:36 minikube kubelet[1367]: I1009 06:29:36.269184    1367 topology_manager.go:212] "Topology Admit Handler"
Oct 09 06:29:36 minikube kubelet[1367]: E1009 06:29:36.269282    1367 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="6621fef7-cc9d-4b80-b0d2-27a9ceea0279" containerName="patch"
Oct 09 06:29:36 minikube kubelet[1367]: E1009 06:29:36.269308    1367 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="6621fef7-cc9d-4b80-b0d2-27a9ceea0279" containerName="patch"
Oct 09 06:29:36 minikube kubelet[1367]: E1009 06:29:36.269323    1367 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="6621fef7-cc9d-4b80-b0d2-27a9ceea0279" containerName="patch"
Oct 09 06:29:36 minikube kubelet[1367]: E1009 06:29:36.269336    1367 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="6621fef7-cc9d-4b80-b0d2-27a9ceea0279" containerName="patch"
Oct 09 06:29:36 minikube kubelet[1367]: E1009 06:29:36.269354    1367 cpu_manager.go:395] "RemoveStaleState: removing container" podUID="857c258a-1ebb-4ad2-abe9-b8c24b334e5e" containerName="create"
Oct 09 06:29:36 minikube kubelet[1367]: I1009 06:29:36.269408    1367 memory_manager.go:346] "RemoveStaleState removing state" podUID="6621fef7-cc9d-4b80-b0d2-27a9ceea0279" containerName="patch"
Oct 09 06:29:36 minikube kubelet[1367]: I1009 06:29:36.269425    1367 memory_manager.go:346] "RemoveStaleState removing state" podUID="6621fef7-cc9d-4b80-b0d2-27a9ceea0279" containerName="patch"
Oct 09 06:29:36 minikube kubelet[1367]: I1009 06:29:36.269436    1367 memory_manager.go:346] "RemoveStaleState removing state" podUID="6621fef7-cc9d-4b80-b0d2-27a9ceea0279" containerName="patch"
Oct 09 06:29:36 minikube kubelet[1367]: I1009 06:29:36.269448    1367 memory_manager.go:346] "RemoveStaleState removing state" podUID="6621fef7-cc9d-4b80-b0d2-27a9ceea0279" containerName="patch"
Oct 09 06:29:36 minikube kubelet[1367]: I1009 06:29:36.269459    1367 memory_manager.go:346] "RemoveStaleState removing state" podUID="6621fef7-cc9d-4b80-b0d2-27a9ceea0279" containerName="patch"
Oct 09 06:29:36 minikube kubelet[1367]: I1009 06:29:36.269470    1367 memory_manager.go:346] "RemoveStaleState removing state" podUID="857c258a-1ebb-4ad2-abe9-b8c24b334e5e" containerName="create"
Oct 09 06:29:36 minikube kubelet[1367]: I1009 06:29:36.336102    1367 reconciler_common.go:258] "operationExecutor.VerifyControllerAttachedVolume started for volume \"kube-api-access-hqhsd\" (UniqueName: \"kubernetes.io/projected/25faf389-9acd-4e3d-b313-2268ba24a35c-kube-api-access-hqhsd\") pod \"task-app-deployment-5f68b76c59-wj49n\" (UID: \"25faf389-9acd-4e3d-b313-2268ba24a35c\") " pod="task-appns/task-app-deployment-5f68b76c59-wj49n"
Oct 09 06:29:41 minikube kubelet[1367]: I1009 06:29:41.115318    1367 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="4f88028726b1f61c13ee6b177b48464187b4566bee67dffcddf05d76778d0522"
Oct 09 06:33:35 minikube kubelet[1367]: E1009 06:33:35.757890    1367 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for kibet2001/task-app-cicd, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="kibet2001/task-app-cicd:v1"
Oct 09 06:33:35 minikube kubelet[1367]: E1009 06:33:35.758448    1367 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for kibet2001/task-app-cicd, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="kibet2001/task-app-cicd:v1"
Oct 09 06:33:35 minikube kubelet[1367]: E1009 06:33:35.758572    1367 kuberuntime_manager.go:1212] container &Container{Name:task-app-task-1,Image:kibet2001/task-app-cicd:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hqhsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod task-app-deployment-5f68b76c59-wj49n_task-appns(25faf389-9acd-4e3d-b313-2268ba24a35c): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for kibet2001/task-app-cicd, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Oct 09 06:33:35 minikube kubelet[1367]: E1009 06:33:35.758622    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-app-task-1\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for kibet2001/task-app-cicd, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="task-appns/task-app-deployment-5f68b76c59-wj49n" podUID=25faf389-9acd-4e3d-b313-2268ba24a35c
Oct 09 06:33:36 minikube kubelet[1367]: E1009 06:33:36.579019    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-app-task-1\" with ImagePullBackOff: \"Back-off pulling image \\\"kibet2001/task-app-cicd:v1\\\"\"" pod="task-appns/task-app-deployment-5f68b76c59-wj49n" podUID=25faf389-9acd-4e3d-b313-2268ba24a35c
Oct 09 06:33:57 minikube kubelet[1367]: E1009 06:33:57.461612    1367 remote_image.go:167] "PullImage from image service failed" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for kibet2001/task-app-cicd, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="kibet2001/task-app-cicd:v1"
Oct 09 06:33:57 minikube kubelet[1367]: E1009 06:33:57.461660    1367 kuberuntime_image.go:53] "Failed to pull image" err="rpc error: code = Unknown desc = Error response from daemon: pull access denied for kibet2001/task-app-cicd, repository does not exist or may require 'docker login': denied: requested access to the resource is denied" image="kibet2001/task-app-cicd:v1"
Oct 09 06:33:57 minikube kubelet[1367]: E1009 06:33:57.461767    1367 kuberuntime_manager.go:1212] container &Container{Name:task-app-task-1,Image:kibet2001/task-app-cicd:v1,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{ContainerPort{Name:,HostPort:0,ContainerPort:8000,Protocol:TCP,HostIP:,},},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hqhsd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},} start failed in pod task-app-deployment-5f68b76c59-wj49n_task-appns(25faf389-9acd-4e3d-b313-2268ba24a35c): ErrImagePull: rpc error: code = Unknown desc = Error response from daemon: pull access denied for kibet2001/task-app-cicd, repository does not exist or may require 'docker login': denied: requested access to the resource is denied
Oct 09 06:33:57 minikube kubelet[1367]: E1009 06:33:57.461823    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-app-task-1\" with ErrImagePull: \"rpc error: code = Unknown desc = Error response from daemon: pull access denied for kibet2001/task-app-cicd, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\"" pod="task-appns/task-app-deployment-5f68b76c59-wj49n" podUID=25faf389-9acd-4e3d-b313-2268ba24a35c
Oct 09 06:34:10 minikube kubelet[1367]: E1009 06:34:10.540990    1367 pod_workers.go:1294] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"task-app-task-1\" with ImagePullBackOff: \"Back-off pulling image \\\"kibet2001/task-app-cicd:v1\\\"\"" pod="task-appns/task-app-deployment-5f68b76c59-wj49n" podUID=25faf389-9acd-4e3d-b313-2268ba24a35c

* 
* ==> storage-provisioner [1b698f878969] <==
* I1009 06:06:22.241586       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F1009 06:06:52.408501       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

* 
* ==> storage-provisioner [7230968ebdf5] <==
* I1009 06:08:26.586655       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
I1009 06:08:26.801213       1 storage_provisioner.go:141] Storage provisioner initialized, now starting service!
I1009 06:08:26.801261       1 leaderelection.go:243] attempting to acquire leader lease kube-system/k8s.io-minikube-hostpath...
I1009 06:08:44.397831       1 leaderelection.go:253] successfully acquired lease kube-system/k8s.io-minikube-hostpath
I1009 06:08:44.397990       1 event.go:282] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"k8s.io-minikube-hostpath", UID:"38605704-0ca4-4e03-9458-a6abf3a4e2b8", APIVersion:"v1", ResourceVersion:"44073", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' minikube_a032f5c8-7cc0-40ce-b6af-013a8825c922 became leader
I1009 06:08:44.398016       1 controller.go:835] Starting provisioner controller k8s.io/minikube-hostpath_minikube_a032f5c8-7cc0-40ce-b6af-013a8825c922!
I1009 06:08:44.516859       1 controller.go:884] Started provisioner controller k8s.io/minikube-hostpath_minikube_a032f5c8-7cc0-40ce-b6af-013a8825c922!

